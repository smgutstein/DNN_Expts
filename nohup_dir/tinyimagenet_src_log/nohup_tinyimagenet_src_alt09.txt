2020-08-28 15:34:43.939245: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-08-28 15:34:44.591156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: Quadro M6000 computeCapability: 5.2
coreClock: 1.114GHz coreCount: 24 deviceMemorySize: 11.93GiB deviceMemoryBandwidth: 295.49GiB/s
2020-08-28 15:34:44.600868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-28 15:34:44.998648: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-28 15:34:45.364174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-28 15:34:45.969436: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-28 15:34:46.300250: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-28 15:34:46.547586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-28 15:34:47.269110: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-28 15:34:47.271589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-08-28 15:34:47.272083: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-08-28 15:34:47.288487: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3100040000 Hz
2020-08-28 15:34:47.293357: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5598bcbce880 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-28 15:34:47.293400: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-28 15:34:47.294712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: Quadro M6000 computeCapability: 5.2
coreClock: 1.114GHz coreCount: 24 deviceMemorySize: 11.93GiB deviceMemoryBandwidth: 295.49GiB/s
2020-08-28 15:34:47.294776: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-28 15:34:47.294802: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-28 15:34:47.294824: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-28 15:34:47.294845: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-28 15:34:47.294866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-28 15:34:47.294888: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-28 15:34:47.294909: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-28 15:34:47.296858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-08-28 15:34:47.296909: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-28 15:34:47.351240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-28 15:34:47.351282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-08-28 15:34:47.351298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-08-28 15:34:47.353906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11498 MB memory) -> physical GPU (device: 0, name: Quadro M6000, pci bus id: 0000:03:00.0, compute capability: 5.2)
2020-08-28 15:34:47.356601: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5598bd0bee00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-28 15:34:47.356625: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro M6000, Compute Capability 5.2
QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-sgutstei'
Qt: XKEYBOARD extension not present on the X server.
2020-08-28 15:35:01.425994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-28 15:35:01.873585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-28 15:35:06.516395: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
CMD LINE ARGS:
config_files : ['./cfg_dir/expt_cfg/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/src_net_1.cfg']
gpu : 
dbg : False
epochs : 0
silent : False
nocheckpoint : False
Using TensorFlow backend.
Saving results to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0
Loading data from dataset_loaders/tinyimagenet200_src_notliving_vs_living
Encoding data
Creating 1-hot encodings -  hot/not_hot (1.0000 / 0.0000)
Creating encoding matrices for train & test data
Preprocessing Train Images
Preprocessing Test Images
Built Data Manager
Standard training
Initializing data manager ...
Using TensorFlow local_backend.
Initializing architecture ...
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(input)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (3, 3), padding="same", strides=(1, 1), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (1, 1), padding="same", strides=(1, 1), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:54: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (3, 3), padding="same", strides=(2, 2), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (1, 1), padding="same", strides=(2, 2), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:79: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:87: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(640, (3, 3), padding="same", strides=(2, 2), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(640, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(640, (1, 1), padding="same", strides=(2, 2), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:104: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(640, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:112: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(640, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:172: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(65, activation="softmax", kernel_regularizer=<keras.reg...)`
  x = Dense(nb_classes, W_regularizer=l2(weight_decay), activation='softmax')(x)
Wide Residual Network-28-10 created.
Starting with weights from epoch 0
Compiling model ...
Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.
Not saving graphical image of net

============================================================

Expt Info:

NB Epochs: 200
Expt Dir: results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0
Expt Prefix: alt09.arl.army.mil_v0

Model:
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   432         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  23040       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  230400      activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  2560        activation_1[0][0]               
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_3[0][0]                   
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         add_1[0][0]                      
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230400      activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230400      activation_4[0][0]               
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         add_2[0][0]                      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230400      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230400      activation_6[0][0]               
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         add_3[0][0]                      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230400      activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230400      activation_8[0][0]               
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 conv2d_10[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         add_4[0][0]                      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  460800      activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  921600      activation_10[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  51200       activation_9[0][0]               
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_12[0][0]                  
                                                                 conv2d_13[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        add_5[0][0]                      
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921600      activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921600      activation_12[0][0]              
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 conv2d_15[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        add_6[0][0]                      
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921600      activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921600      activation_14[0][0]              
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 conv2d_17[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        add_7[0][0]                      
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921600      activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921600      activation_16[0][0]              
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 conv2d_19[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        add_8[0][0]                      
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    1843200     activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    3686400     activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    204800      activation_17[0][0]              
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_21[0][0]                  
                                                                 conv2d_22[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        add_9[0][0]                      
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3686400     activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3686400     activation_20[0][0]              
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 conv2d_24[0][0]                  
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        add_10[0][0]                     
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3686400     activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3686400     activation_22[0][0]              
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 conv2d_26[0][0]                  
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        add_11[0][0]                     
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3686400     activation_23[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3686400     activation_24[0][0]              
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 conv2d_28[0][0]                  
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        add_12[0][0]                     
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 1, 1, 640)    0           activation_25[0][0]              
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 640)          0           average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 65)           41665       flatten_1[0][0]                  
==================================================================================================
Total params: 36,532,401
Trainable params: 36,514,449
Non-trainable params: 17,952
__________________________________________________________________________________________________

============================================================


3: +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  
7: -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  
8: -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  
9: -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  
17: -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
18: -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
22: -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
24: -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
25: -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
26: -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
37: -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
38: -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
41: -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
42: -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
48: -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
49: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
50: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
54: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
58: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
62: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
64: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
68: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
69: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
73: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
76: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
77: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
82: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
87: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
88: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
89: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
91: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
96: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
98: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
100: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
101: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
102: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
104: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
109: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
112: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
113: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
116: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
118: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
123: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
129: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
130: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
132: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
133: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
134: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
138: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
141: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
143: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
145: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
148: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  
     -  -  -  -  -  
150: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  
     -  -  -  -  -  
152: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  
     -  -  -  -  -  
157: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  
     -  -  -  -  -  
160: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  
     -  -  -  -  -  
175: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  
     -  -  -  -  -  
178: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  
     -  -  -  -  -  
184: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  
     -  -  -  -  -  
186: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     +  -  -  -  -  
187: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  +  -  -  -  
191: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  +  -  -  
194: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  +  -  
197: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  +  





Total Classes: 65

3:miniskirt -- 7:obelisk -- 8:plunger -- 9:bucket -- 17:cannon -- 
18:broom -- 22:chain -- 24:espresso -- 25:seashore -- 26:organ -- 
37:torch -- 38:tractor -- 41:desk -- 42:beacon -- 48:reel -- 
49:iPod -- 50:abacus -- 54:dam -- 58:cliff -- 62:teapot -- 
64:sunglasses -- 68:maypole -- 69:jinrikisha -- 73:barn -- 76:snorkel -- 
77:turnstile -- 82:apron -- 87:potpie -- 88:lampshade -- 89:volleyball -- 
91:lifeboat -- 96:crane -- 98:pretzel -- 100:oboe -- 101:syringe -- 
102:birdhouse -- 104:fountain -- 109:barrel -- 112:alp -- 113:lakeside -- 
116:vestment -- 118:gasmask -- 123:flagpole -- 129:wok -- 130:sandal -- 
132:beaker -- 133:candle -- 134:basketball -- 138:nail -- 141:thatch -- 
143:hourglass -- 145:plate -- 148:refrigerator -- 150:trolleybus -- 152:umbrella -- 
157:dumbbell -- 160:sock -- 175:backpack -- 178:cardigan -- 184:pizza -- 
186:limousine -- 187:stopwatch -- 191:chest -- 194:sombrero -- 197:projectile -- 

Built Net Manager

Init loss and acc:                             loss:  14.33943 - acc_top_1: 0.01603 - val_loss: 14.33943 - val_acc_top_1: 0.01600
Saving results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_encodings_0.pkl
Epoch 00000: val_acc_top_1 improved from -inf to 0.01600, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_0.h5

========================================================================

Epoch 1/200
lr =  0.1

Epoch 00001: LearningRateScheduler setting learning rate to 0.1.
 - 209s - loss: 11.9455 - acc_top_1: 0.0663 - val_loss: 9.7480 - val_acc_top_1: 0.1342
Epoch 00001: val_acc_top_1 improved from 0.01600 to 0.13415, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_1.h5

========================================================================

Epoch 2/200
lr =  0.1

Epoch 00002: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 8.2271 - acc_top_1: 0.1653 - val_loss: 6.7718 - val_acc_top_1: 0.2074
Epoch 00002: val_acc_top_1 improved from 0.13415 to 0.20738, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_2.h5

========================================================================

Epoch 3/200
lr =  0.1

Epoch 00003: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 6.0513 - acc_top_1: 0.2324 - val_loss: 5.6702 - val_acc_top_1: 0.2526
Epoch 00003: val_acc_top_1 improved from 0.20738 to 0.25262, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_3.h5

========================================================================

Epoch 4/200
lr =  0.1

Epoch 00004: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 4.7473 - acc_top_1: 0.2847 - val_loss: 4.7481 - val_acc_top_1: 0.2954
Epoch 00004: val_acc_top_1 improved from 0.25262 to 0.29538, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_4.h5

========================================================================

Epoch 5/200
lr =  0.1

Epoch 00005: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 3.9502 - acc_top_1: 0.3214 - val_loss: 3.9111 - val_acc_top_1: 0.3357
Epoch 00005: val_acc_top_1 improved from 0.29538 to 0.33569, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_5.h5

========================================================================

Epoch 6/200
lr =  0.1

Epoch 00006: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 3.4637 - acc_top_1: 0.3585 - val_loss: 3.0430 - val_acc_top_1: 0.3542
Epoch 00006: val_acc_top_1 improved from 0.33569 to 0.35415, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_6.h5

========================================================================

Epoch 7/200
lr =  0.1

Epoch 00007: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 3.1476 - acc_top_1: 0.3899 - val_loss: 3.3007 - val_acc_top_1: 0.3920
Epoch 00007: val_acc_top_1 improved from 0.35415 to 0.39200, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_7.h5

========================================================================

Epoch 8/200
lr =  0.1

Epoch 00008: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.9675 - acc_top_1: 0.4096 - val_loss: 2.9499 - val_acc_top_1: 0.4089
Epoch 00008: val_acc_top_1 improved from 0.39200 to 0.40892, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_8.h5

========================================================================

Epoch 9/200
lr =  0.1

Epoch 00009: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8416 - acc_top_1: 0.4325 - val_loss: 3.2399 - val_acc_top_1: 0.4274
Epoch 00009: val_acc_top_1 improved from 0.40892 to 0.42738, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_9.h5

========================================================================

Epoch 10/200
lr =  0.1

Epoch 00010: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7735 - acc_top_1: 0.4505 - val_loss: 2.8020 - val_acc_top_1: 0.4209
Epoch 00010: val_acc_top_1 did not improve from 0.42738

========================================================================

Epoch 11/200
lr =  0.1

Epoch 00011: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7215 - acc_top_1: 0.4652 - val_loss: 2.9575 - val_acc_top_1: 0.4375
Epoch 00011: val_acc_top_1 improved from 0.42738 to 0.43754, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_11.h5

========================================================================

Epoch 12/200
lr =  0.1

Epoch 00012: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7001 - acc_top_1: 0.4765 - val_loss: 2.4229 - val_acc_top_1: 0.4652
Epoch 00012: val_acc_top_1 improved from 0.43754 to 0.46523, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_12.h5

========================================================================

Epoch 13/200
lr =  0.1

Epoch 00013: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6751 - acc_top_1: 0.4871 - val_loss: 2.9595 - val_acc_top_1: 0.4603
Epoch 00013: val_acc_top_1 did not improve from 0.46523

========================================================================

Epoch 14/200
lr =  0.1

Epoch 00014: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6621 - acc_top_1: 0.5001 - val_loss: 3.1511 - val_acc_top_1: 0.4689
Epoch 00014: val_acc_top_1 improved from 0.46523 to 0.46892, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_14.h5

========================================================================

Epoch 15/200
lr =  0.1

Epoch 00015: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6461 - acc_top_1: 0.5099 - val_loss: 2.8197 - val_acc_top_1: 0.4717
Epoch 00015: val_acc_top_1 improved from 0.46892 to 0.47169, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_15.h5

========================================================================

Epoch 16/200
lr =  0.1

Epoch 00016: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6503 - acc_top_1: 0.5165 - val_loss: 2.9952 - val_acc_top_1: 0.4628
Epoch 00016: val_acc_top_1 did not improve from 0.47169

========================================================================

Epoch 17/200
lr =  0.1

Epoch 00017: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6524 - acc_top_1: 0.5234 - val_loss: 2.9931 - val_acc_top_1: 0.4668
Epoch 00017: val_acc_top_1 did not improve from 0.47169

========================================================================

Epoch 18/200
lr =  0.1

Epoch 00018: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6562 - acc_top_1: 0.5280 - val_loss: 2.5933 - val_acc_top_1: 0.4877
Epoch 00018: val_acc_top_1 improved from 0.47169 to 0.48769, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_18.h5

========================================================================

Epoch 19/200
lr =  0.1

Epoch 00019: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6610 - acc_top_1: 0.5370 - val_loss: 3.2092 - val_acc_top_1: 0.4794
Epoch 00019: val_acc_top_1 did not improve from 0.48769

========================================================================

Epoch 20/200
lr =  0.1

Epoch 00020: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6557 - acc_top_1: 0.5466 - val_loss: 2.9525 - val_acc_top_1: 0.4902
Epoch 00020: val_acc_top_1 improved from 0.48769 to 0.49015, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_20.h5

========================================================================

Epoch 21/200
lr =  0.1

Epoch 00021: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6611 - acc_top_1: 0.5472 - val_loss: 3.5285 - val_acc_top_1: 0.4754
Epoch 00021: val_acc_top_1 did not improve from 0.49015

========================================================================

Epoch 22/200
lr =  0.1

Epoch 00022: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6724 - acc_top_1: 0.5521 - val_loss: 3.1868 - val_acc_top_1: 0.4892
Epoch 00022: val_acc_top_1 did not improve from 0.49015

========================================================================

Epoch 23/200
lr =  0.1

Epoch 00023: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6610 - acc_top_1: 0.5576 - val_loss: 2.6636 - val_acc_top_1: 0.4834
Epoch 00023: val_acc_top_1 did not improve from 0.49015

========================================================================

Epoch 24/200
lr =  0.1

Epoch 00024: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6826 - acc_top_1: 0.5587 - val_loss: 3.2025 - val_acc_top_1: 0.4978
Epoch 00024: val_acc_top_1 improved from 0.49015 to 0.49785, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_24.h5

========================================================================

Epoch 25/200
lr =  0.1

Epoch 00025: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6678 - acc_top_1: 0.5666 - val_loss: 2.9620 - val_acc_top_1: 0.5046
Epoch 00025: val_acc_top_1 improved from 0.49785 to 0.50462, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_25.h5

========================================================================

Epoch 26/200
lr =  0.1

Epoch 00026: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6716 - acc_top_1: 0.5714 - val_loss: 3.0868 - val_acc_top_1: 0.4963
Epoch 00026: val_acc_top_1 did not improve from 0.50462

========================================================================

Epoch 27/200
lr =  0.1

Epoch 00027: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6899 - acc_top_1: 0.5735 - val_loss: 2.8027 - val_acc_top_1: 0.4938
Epoch 00027: val_acc_top_1 did not improve from 0.50462

========================================================================

Epoch 28/200
lr =  0.1

Epoch 00028: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.6975 - acc_top_1: 0.5742 - val_loss: 2.8616 - val_acc_top_1: 0.5117
Epoch 00028: val_acc_top_1 improved from 0.50462 to 0.51169, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_28.h5

========================================================================

Epoch 29/200
lr =  0.1

Epoch 00029: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7039 - acc_top_1: 0.5787 - val_loss: 3.1035 - val_acc_top_1: 0.5098
Epoch 00029: val_acc_top_1 did not improve from 0.51169

========================================================================

Epoch 30/200
lr =  0.1

Epoch 00030: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7074 - acc_top_1: 0.5792 - val_loss: 3.0279 - val_acc_top_1: 0.5040
Epoch 00030: val_acc_top_1 did not improve from 0.51169

========================================================================

Epoch 31/200
lr =  0.1

Epoch 00031: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7123 - acc_top_1: 0.5857 - val_loss: 2.9914 - val_acc_top_1: 0.5040
Epoch 00031: val_acc_top_1 did not improve from 0.51169

========================================================================

Epoch 32/200
lr =  0.1

Epoch 00032: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7103 - acc_top_1: 0.5889 - val_loss: 2.8521 - val_acc_top_1: 0.4929
Epoch 00032: val_acc_top_1 did not improve from 0.51169

========================================================================

Epoch 33/200
lr =  0.1

Epoch 00033: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7208 - acc_top_1: 0.5919 - val_loss: 2.9929 - val_acc_top_1: 0.4960
Epoch 00033: val_acc_top_1 did not improve from 0.51169

========================================================================

Epoch 34/200
lr =  0.1

Epoch 00034: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7232 - acc_top_1: 0.5966 - val_loss: 3.5206 - val_acc_top_1: 0.4831
Epoch 00034: val_acc_top_1 did not improve from 0.51169

========================================================================

Epoch 35/200
lr =  0.1

Epoch 00035: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7383 - acc_top_1: 0.5949 - val_loss: 3.9157 - val_acc_top_1: 0.4988
Epoch 00035: val_acc_top_1 did not improve from 0.51169

========================================================================

Epoch 36/200
lr =  0.1

Epoch 00036: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7523 - acc_top_1: 0.5954 - val_loss: 3.4294 - val_acc_top_1: 0.5065
Epoch 00036: val_acc_top_1 did not improve from 0.51169

========================================================================

Epoch 37/200
lr =  0.1

Epoch 00037: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7394 - acc_top_1: 0.6020 - val_loss: 3.6427 - val_acc_top_1: 0.5055
Epoch 00037: val_acc_top_1 did not improve from 0.51169

========================================================================

Epoch 38/200
lr =  0.1

Epoch 00038: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7552 - acc_top_1: 0.6008 - val_loss: 3.3868 - val_acc_top_1: 0.5215
Epoch 00038: val_acc_top_1 improved from 0.51169 to 0.52154, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_38.h5

========================================================================

Epoch 39/200
lr =  0.1

Epoch 00039: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7663 - acc_top_1: 0.6034 - val_loss: 3.6838 - val_acc_top_1: 0.5052
Epoch 00039: val_acc_top_1 did not improve from 0.52154

========================================================================

Epoch 40/200
lr =  0.1

Epoch 00040: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7707 - acc_top_1: 0.6065 - val_loss: 3.4647 - val_acc_top_1: 0.5105
Epoch 00040: val_acc_top_1 did not improve from 0.52154

========================================================================

Epoch 41/200
lr =  0.1

Epoch 00041: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7781 - acc_top_1: 0.6066 - val_loss: 2.9852 - val_acc_top_1: 0.5132
Epoch 00041: val_acc_top_1 did not improve from 0.52154

========================================================================

Epoch 42/200
lr =  0.1

Epoch 00042: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7843 - acc_top_1: 0.6087 - val_loss: 3.4541 - val_acc_top_1: 0.5169
Epoch 00042: val_acc_top_1 did not improve from 0.52154

========================================================================

Epoch 43/200
lr =  0.1

Epoch 00043: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7739 - acc_top_1: 0.6137 - val_loss: 3.3638 - val_acc_top_1: 0.5080
Epoch 00043: val_acc_top_1 did not improve from 0.52154

========================================================================

Epoch 44/200
lr =  0.1

Epoch 00044: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7882 - acc_top_1: 0.6117 - val_loss: 3.3235 - val_acc_top_1: 0.5268
Epoch 00044: val_acc_top_1 improved from 0.52154 to 0.52677, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_44.h5

========================================================================

Epoch 45/200
lr =  0.1

Epoch 00045: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8014 - acc_top_1: 0.6124 - val_loss: 3.6962 - val_acc_top_1: 0.5145
Epoch 00045: val_acc_top_1 did not improve from 0.52677

========================================================================

Epoch 46/200
lr =  0.1

Epoch 00046: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.7917 - acc_top_1: 0.6166 - val_loss: 3.7961 - val_acc_top_1: 0.5151
Epoch 00046: val_acc_top_1 did not improve from 0.52677

========================================================================

Epoch 47/200
lr =  0.1

Epoch 00047: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8011 - acc_top_1: 0.6163 - val_loss: 3.5703 - val_acc_top_1: 0.5169
Epoch 00047: val_acc_top_1 did not improve from 0.52677

========================================================================

Epoch 48/200
lr =  0.1

Epoch 00048: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8009 - acc_top_1: 0.6183 - val_loss: 3.0335 - val_acc_top_1: 0.5252
Epoch 00048: val_acc_top_1 did not improve from 0.52677

========================================================================

Epoch 49/200
lr =  0.1

Epoch 00049: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8054 - acc_top_1: 0.6215 - val_loss: 3.4820 - val_acc_top_1: 0.5206
Epoch 00049: val_acc_top_1 did not improve from 0.52677

========================================================================

Epoch 50/200
lr =  0.1

Epoch 00050: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8187 - acc_top_1: 0.6208 - val_loss: 3.2142 - val_acc_top_1: 0.5203
Epoch 00050: val_acc_top_1 did not improve from 0.52677

========================================================================

Epoch 51/200
lr =  0.1

Epoch 00051: LearningRateScheduler setting learning rate to 0.1.
 - 199s - loss: 2.8204 - acc_top_1: 0.6183 - val_loss: 3.3502 - val_acc_top_1: 0.5194
Epoch 00051: val_acc_top_1 did not improve from 0.52677

========================================================================

Epoch 52/200
lr =  0.1

Epoch 00052: LearningRateScheduler setting learning rate to 0.1.
 - 199s - loss: 2.8295 - acc_top_1: 0.6245 - val_loss: 3.5698 - val_acc_top_1: 0.5111
Epoch 00052: val_acc_top_1 did not improve from 0.52677

========================================================================

Epoch 53/200
lr =  0.1

Epoch 00053: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8207 - acc_top_1: 0.6279 - val_loss: 3.3805 - val_acc_top_1: 0.5095
Epoch 00053: val_acc_top_1 did not improve from 0.52677

========================================================================

Epoch 54/200
lr =  0.1

Epoch 00054: LearningRateScheduler setting learning rate to 0.1.
 - 199s - loss: 2.8386 - acc_top_1: 0.6260 - val_loss: 3.1907 - val_acc_top_1: 0.5271
Epoch 00054: val_acc_top_1 improved from 0.52677 to 0.52708, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_54.h5

========================================================================

Epoch 55/200
lr =  0.1

Epoch 00055: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8388 - acc_top_1: 0.6267 - val_loss: 3.2738 - val_acc_top_1: 0.5062
Epoch 00055: val_acc_top_1 did not improve from 0.52708

========================================================================

Epoch 56/200
lr =  0.1

Epoch 00056: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8286 - acc_top_1: 0.6307 - val_loss: 2.9789 - val_acc_top_1: 0.5117
Epoch 00056: val_acc_top_1 did not improve from 0.52708

========================================================================

Epoch 57/200
lr =  0.1

Epoch 00057: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8445 - acc_top_1: 0.6309 - val_loss: 3.5248 - val_acc_top_1: 0.5129
Epoch 00057: val_acc_top_1 did not improve from 0.52708

========================================================================

Epoch 58/200
lr =  0.1

Epoch 00058: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8458 - acc_top_1: 0.6303 - val_loss: 3.5555 - val_acc_top_1: 0.5058
Epoch 00058: val_acc_top_1 did not improve from 0.52708

========================================================================

Epoch 59/200
lr =  0.1

Epoch 00059: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8538 - acc_top_1: 0.6277 - val_loss: 3.5113 - val_acc_top_1: 0.5018
Epoch 00059: val_acc_top_1 did not improve from 0.52708

========================================================================

Epoch 60/200
lr =  0.1

Epoch 00060: LearningRateScheduler setting learning rate to 0.1.
 - 200s - loss: 2.8477 - acc_top_1: 0.6362 - val_loss: 3.2474 - val_acc_top_1: 0.5028
Epoch 00060: val_acc_top_1 did not improve from 0.52708

========================================================================

Epoch 61/200
lr =  0.020000000000000004

Epoch 00061: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 2.3267 - acc_top_1: 0.7686 - val_loss: 2.8082 - val_acc_top_1: 0.5994
Epoch 00061: val_acc_top_1 improved from 0.52708 to 0.59938, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_61.h5

========================================================================

Epoch 62/200
lr =  0.020000000000000004

Epoch 00062: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.9117 - acc_top_1: 0.8533 - val_loss: 2.8947 - val_acc_top_1: 0.5991
Epoch 00062: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 63/200
lr =  0.020000000000000004

Epoch 00063: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.6901 - acc_top_1: 0.8870 - val_loss: 3.1422 - val_acc_top_1: 0.5966
Epoch 00063: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 64/200
lr =  0.020000000000000004

Epoch 00064: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.5180 - acc_top_1: 0.9132 - val_loss: 2.9740 - val_acc_top_1: 0.5898
Epoch 00064: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 65/200
lr =  0.020000000000000004

Epoch 00065: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.3961 - acc_top_1: 0.9247 - val_loss: 3.1903 - val_acc_top_1: 0.5815
Epoch 00065: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 66/200
lr =  0.020000000000000004

Epoch 00066: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2967 - acc_top_1: 0.9344 - val_loss: 3.0593 - val_acc_top_1: 0.5788
Epoch 00066: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 67/200
lr =  0.020000000000000004

Epoch 00067: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2370 - acc_top_1: 0.9381 - val_loss: 2.8123 - val_acc_top_1: 0.5732
Epoch 00067: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 68/200
lr =  0.020000000000000004

Epoch 00068: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1978 - acc_top_1: 0.9364 - val_loss: 2.7746 - val_acc_top_1: 0.5652
Epoch 00068: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 69/200
lr =  0.020000000000000004

Epoch 00069: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1793 - acc_top_1: 0.9318 - val_loss: 2.7611 - val_acc_top_1: 0.5582
Epoch 00069: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 70/200
lr =  0.020000000000000004

Epoch 00070: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 199s - loss: 1.1742 - acc_top_1: 0.9266 - val_loss: 3.7312 - val_acc_top_1: 0.5671
Epoch 00070: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 71/200
lr =  0.020000000000000004

Epoch 00071: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 199s - loss: 1.1709 - acc_top_1: 0.9230 - val_loss: 3.1002 - val_acc_top_1: 0.5622
Epoch 00071: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 72/200
lr =  0.020000000000000004

Epoch 00072: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1777 - acc_top_1: 0.9193 - val_loss: 2.1383 - val_acc_top_1: 0.5542
Epoch 00072: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 73/200
lr =  0.020000000000000004

Epoch 00073: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 199s - loss: 1.1706 - acc_top_1: 0.9218 - val_loss: 2.7045 - val_acc_top_1: 0.5585
Epoch 00073: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 74/200
lr =  0.020000000000000004

Epoch 00074: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1738 - acc_top_1: 0.9194 - val_loss: 3.1741 - val_acc_top_1: 0.5483
Epoch 00074: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 75/200
lr =  0.020000000000000004

Epoch 00075: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 199s - loss: 1.1618 - acc_top_1: 0.9232 - val_loss: 3.0617 - val_acc_top_1: 0.5566
Epoch 00075: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 76/200
lr =  0.020000000000000004

Epoch 00076: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1638 - acc_top_1: 0.9242 - val_loss: 2.9826 - val_acc_top_1: 0.5542
Epoch 00076: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 77/200
lr =  0.020000000000000004

Epoch 00077: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1617 - acc_top_1: 0.9241 - val_loss: 3.4796 - val_acc_top_1: 0.5643
Epoch 00077: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 78/200
lr =  0.020000000000000004

Epoch 00078: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1653 - acc_top_1: 0.9245 - val_loss: 3.1285 - val_acc_top_1: 0.5440
Epoch 00078: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 79/200
lr =  0.020000000000000004

Epoch 00079: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1626 - acc_top_1: 0.9273 - val_loss: 2.9100 - val_acc_top_1: 0.5566
Epoch 00079: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 80/200
lr =  0.020000000000000004

Epoch 00080: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1580 - acc_top_1: 0.9280 - val_loss: 2.9997 - val_acc_top_1: 0.5489
Epoch 00080: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 81/200
lr =  0.020000000000000004

Epoch 00081: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 199s - loss: 1.1719 - acc_top_1: 0.9243 - val_loss: 2.9372 - val_acc_top_1: 0.5437
Epoch 00081: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 82/200
lr =  0.020000000000000004

Epoch 00082: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 199s - loss: 1.1688 - acc_top_1: 0.9266 - val_loss: 3.3419 - val_acc_top_1: 0.5363
Epoch 00082: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 83/200
lr =  0.020000000000000004

Epoch 00083: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1737 - acc_top_1: 0.9266 - val_loss: 2.9957 - val_acc_top_1: 0.5388
Epoch 00083: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 84/200
lr =  0.020000000000000004

Epoch 00084: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1645 - acc_top_1: 0.9314 - val_loss: 2.9965 - val_acc_top_1: 0.5542
Epoch 00084: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 85/200
lr =  0.020000000000000004

Epoch 00085: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1728 - acc_top_1: 0.9294 - val_loss: 2.7816 - val_acc_top_1: 0.5560
Epoch 00085: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 86/200
lr =  0.020000000000000004

Epoch 00086: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1528 - acc_top_1: 0.9346 - val_loss: 3.1778 - val_acc_top_1: 0.5406
Epoch 00086: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 87/200
lr =  0.020000000000000004

Epoch 00087: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1866 - acc_top_1: 0.9259 - val_loss: 3.1628 - val_acc_top_1: 0.5458
Epoch 00087: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 88/200
lr =  0.020000000000000004

Epoch 00088: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1834 - acc_top_1: 0.9285 - val_loss: 2.8472 - val_acc_top_1: 0.5455
Epoch 00088: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 89/200
lr =  0.020000000000000004

Epoch 00089: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1631 - acc_top_1: 0.9355 - val_loss: 3.4322 - val_acc_top_1: 0.5449
Epoch 00089: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 90/200
lr =  0.020000000000000004

Epoch 00090: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1674 - acc_top_1: 0.9335 - val_loss: 2.3488 - val_acc_top_1: 0.5437
Epoch 00090: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 91/200
lr =  0.020000000000000004

Epoch 00091: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1843 - acc_top_1: 0.9286 - val_loss: 2.8453 - val_acc_top_1: 0.5428
Epoch 00091: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 92/200
lr =  0.020000000000000004

Epoch 00092: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1892 - acc_top_1: 0.9286 - val_loss: 2.9094 - val_acc_top_1: 0.5351
Epoch 00092: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 93/200
lr =  0.020000000000000004

Epoch 00093: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1620 - acc_top_1: 0.9377 - val_loss: 3.3750 - val_acc_top_1: 0.5489
Epoch 00093: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 94/200
lr =  0.020000000000000004

Epoch 00094: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1892 - acc_top_1: 0.9278 - val_loss: 3.0660 - val_acc_top_1: 0.5514
Epoch 00094: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 95/200
lr =  0.020000000000000004

Epoch 00095: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1806 - acc_top_1: 0.9333 - val_loss: 3.0853 - val_acc_top_1: 0.5431
Epoch 00095: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 96/200
lr =  0.020000000000000004

Epoch 00096: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 199s - loss: 1.1922 - acc_top_1: 0.9310 - val_loss: 3.9108 - val_acc_top_1: 0.5471
Epoch 00096: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 97/200
lr =  0.020000000000000004

Epoch 00097: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1859 - acc_top_1: 0.9333 - val_loss: 2.7741 - val_acc_top_1: 0.5474
Epoch 00097: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 98/200
lr =  0.020000000000000004

Epoch 00098: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1597 - acc_top_1: 0.9417 - val_loss: 2.9624 - val_acc_top_1: 0.5369
Epoch 00098: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 99/200
lr =  0.020000000000000004

Epoch 00099: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 199s - loss: 1.1750 - acc_top_1: 0.9357 - val_loss: 3.1978 - val_acc_top_1: 0.5425
Epoch 00099: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 100/200
lr =  0.020000000000000004

Epoch 00100: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 199s - loss: 1.1880 - acc_top_1: 0.9316 - val_loss: 3.9131 - val_acc_top_1: 0.5412
Epoch 00100: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 101/200
lr =  0.020000000000000004

Epoch 00101: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1974 - acc_top_1: 0.9327 - val_loss: 3.4965 - val_acc_top_1: 0.5422
Epoch 00101: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 102/200
lr =  0.020000000000000004

Epoch 00102: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1718 - acc_top_1: 0.9391 - val_loss: 2.8171 - val_acc_top_1: 0.5360
Epoch 00102: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 103/200
lr =  0.020000000000000004

Epoch 00103: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1881 - acc_top_1: 0.9350 - val_loss: 3.8977 - val_acc_top_1: 0.5246
Epoch 00103: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 104/200
lr =  0.020000000000000004

Epoch 00104: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1810 - acc_top_1: 0.9370 - val_loss: 2.9743 - val_acc_top_1: 0.5388
Epoch 00104: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 105/200
lr =  0.020000000000000004

Epoch 00105: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1953 - acc_top_1: 0.9328 - val_loss: 3.6821 - val_acc_top_1: 0.5486
Epoch 00105: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 106/200
lr =  0.020000000000000004

Epoch 00106: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2169 - acc_top_1: 0.9299 - val_loss: 3.1977 - val_acc_top_1: 0.5252
Epoch 00106: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 107/200
lr =  0.020000000000000004

Epoch 00107: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2100 - acc_top_1: 0.9332 - val_loss: 3.3139 - val_acc_top_1: 0.5511
Epoch 00107: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 108/200
lr =  0.020000000000000004

Epoch 00108: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1889 - acc_top_1: 0.9391 - val_loss: 3.6076 - val_acc_top_1: 0.5440
Epoch 00108: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 109/200
lr =  0.020000000000000004

Epoch 00109: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1756 - acc_top_1: 0.9415 - val_loss: 2.9427 - val_acc_top_1: 0.5523
Epoch 00109: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 110/200
lr =  0.020000000000000004

Epoch 00110: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2014 - acc_top_1: 0.9332 - val_loss: 2.9664 - val_acc_top_1: 0.5508
Epoch 00110: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 111/200
lr =  0.020000000000000004

Epoch 00111: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1977 - acc_top_1: 0.9362 - val_loss: 3.0080 - val_acc_top_1: 0.5345
Epoch 00111: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 112/200
lr =  0.020000000000000004

Epoch 00112: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2169 - acc_top_1: 0.9310 - val_loss: 2.4531 - val_acc_top_1: 0.5394
Epoch 00112: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 113/200
lr =  0.020000000000000004

Epoch 00113: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2120 - acc_top_1: 0.9334 - val_loss: 3.4957 - val_acc_top_1: 0.5403
Epoch 00113: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 114/200
lr =  0.020000000000000004

Epoch 00114: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2184 - acc_top_1: 0.9333 - val_loss: 3.4998 - val_acc_top_1: 0.5486
Epoch 00114: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 115/200
lr =  0.020000000000000004

Epoch 00115: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2121 - acc_top_1: 0.9351 - val_loss: 4.1586 - val_acc_top_1: 0.5289
Epoch 00115: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 116/200
lr =  0.020000000000000004

Epoch 00116: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2099 - acc_top_1: 0.9374 - val_loss: 3.9438 - val_acc_top_1: 0.5378
Epoch 00116: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 117/200
lr =  0.020000000000000004

Epoch 00117: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2020 - acc_top_1: 0.9386 - val_loss: 3.6101 - val_acc_top_1: 0.5422
Epoch 00117: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 118/200
lr =  0.020000000000000004

Epoch 00118: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 199s - loss: 1.2076 - acc_top_1: 0.9367 - val_loss: 3.5895 - val_acc_top_1: 0.5366
Epoch 00118: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 119/200
lr =  0.020000000000000004

Epoch 00119: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 199s - loss: 1.2163 - acc_top_1: 0.9349 - val_loss: 2.3992 - val_acc_top_1: 0.5449
Epoch 00119: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 120/200
lr =  0.020000000000000004

Epoch 00120: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.2212 - acc_top_1: 0.9346 - val_loss: 3.3986 - val_acc_top_1: 0.5237
Epoch 00120: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 121/200
lr =  0.004000000000000001

Epoch 00121: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 1.0885 - acc_top_1: 0.9760 - val_loss: 2.4716 - val_acc_top_1: 0.5732
Epoch 00121: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 122/200
lr =  0.004000000000000001

Epoch 00122: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 1.0061 - acc_top_1: 0.9953 - val_loss: 2.2588 - val_acc_top_1: 0.5880
Epoch 00122: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 123/200
lr =  0.004000000000000001

Epoch 00123: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.9783 - acc_top_1: 0.9972 - val_loss: 2.6284 - val_acc_top_1: 0.5794
Epoch 00123: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 124/200
lr =  0.004000000000000001

Epoch 00124: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.9547 - acc_top_1: 0.9984 - val_loss: 2.3589 - val_acc_top_1: 0.5942
Epoch 00124: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 125/200
lr =  0.004000000000000001

Epoch 00125: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.9343 - acc_top_1: 0.9983 - val_loss: 2.6013 - val_acc_top_1: 0.5862
Epoch 00125: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 126/200
lr =  0.004000000000000001

Epoch 00126: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.9146 - acc_top_1: 0.9990 - val_loss: 2.7442 - val_acc_top_1: 0.5800
Epoch 00126: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 127/200
lr =  0.004000000000000001

Epoch 00127: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.8953 - acc_top_1: 0.9988 - val_loss: 2.9316 - val_acc_top_1: 0.5917
Epoch 00127: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 128/200
lr =  0.004000000000000001

Epoch 00128: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.8781 - acc_top_1: 0.9987 - val_loss: 3.1579 - val_acc_top_1: 0.5858
Epoch 00128: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 129/200
lr =  0.004000000000000001

Epoch 00129: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 199s - loss: 0.8593 - acc_top_1: 0.9994 - val_loss: 2.8143 - val_acc_top_1: 0.5877
Epoch 00129: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 130/200
lr =  0.004000000000000001

Epoch 00130: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 199s - loss: 0.8423 - acc_top_1: 0.9994 - val_loss: 2.8332 - val_acc_top_1: 0.5846
Epoch 00130: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 131/200
lr =  0.004000000000000001

Epoch 00131: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 199s - loss: 0.8247 - acc_top_1: 0.9997 - val_loss: 2.2539 - val_acc_top_1: 0.5831
Epoch 00131: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 132/200
lr =  0.004000000000000001

Epoch 00132: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 199s - loss: 0.8088 - acc_top_1: 0.9995 - val_loss: 2.2595 - val_acc_top_1: 0.5929
Epoch 00132: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 133/200
lr =  0.004000000000000001

Epoch 00133: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 199s - loss: 0.7924 - acc_top_1: 0.9996 - val_loss: 2.8417 - val_acc_top_1: 0.5938
Epoch 00133: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 134/200
lr =  0.004000000000000001

Epoch 00134: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.7763 - acc_top_1: 0.9997 - val_loss: 3.0631 - val_acc_top_1: 0.5905
Epoch 00134: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 135/200
lr =  0.004000000000000001

Epoch 00135: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.7610 - acc_top_1: 0.9998 - val_loss: 2.4705 - val_acc_top_1: 0.5926
Epoch 00135: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 136/200
lr =  0.004000000000000001

Epoch 00136: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.7455 - acc_top_1: 0.9998 - val_loss: 1.9447 - val_acc_top_1: 0.5920
Epoch 00136: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 137/200
lr =  0.004000000000000001

Epoch 00137: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.7306 - acc_top_1: 0.9998 - val_loss: 3.0616 - val_acc_top_1: 0.5985
Epoch 00137: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 138/200
lr =  0.004000000000000001

Epoch 00138: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.7162 - acc_top_1: 0.9999 - val_loss: 2.7876 - val_acc_top_1: 0.5846
Epoch 00138: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 139/200
lr =  0.004000000000000001

Epoch 00139: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 199s - loss: 0.7023 - acc_top_1: 0.9998 - val_loss: 2.1974 - val_acc_top_1: 0.5957
Epoch 00139: val_acc_top_1 did not improve from 0.59938

========================================================================

Epoch 140/200
lr =  0.004000000000000001

Epoch 00140: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 199s - loss: 0.6888 - acc_top_1: 0.9995 - val_loss: 3.0942 - val_acc_top_1: 0.6052
Epoch 00140: val_acc_top_1 improved from 0.59938 to 0.60523, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_140.h5

========================================================================

Epoch 141/200
lr =  0.004000000000000001

Epoch 00141: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 199s - loss: 0.6750 - acc_top_1: 0.9998 - val_loss: 3.2403 - val_acc_top_1: 0.6037
Epoch 00141: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 142/200
lr =  0.004000000000000001

Epoch 00142: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.6615 - acc_top_1: 0.9998 - val_loss: 2.4059 - val_acc_top_1: 0.5917
Epoch 00142: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 143/200
lr =  0.004000000000000001

Epoch 00143: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 199s - loss: 0.6483 - acc_top_1: 0.9998 - val_loss: 2.2863 - val_acc_top_1: 0.5951
Epoch 00143: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 144/200
lr =  0.004000000000000001

Epoch 00144: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.6357 - acc_top_1: 0.9998 - val_loss: 1.6876 - val_acc_top_1: 0.5969
Epoch 00144: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 145/200
lr =  0.004000000000000001

Epoch 00145: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.6230 - acc_top_1: 0.9998 - val_loss: 2.1353 - val_acc_top_1: 0.5880
Epoch 00145: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 146/200
lr =  0.004000000000000001

Epoch 00146: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.6108 - acc_top_1: 0.9998 - val_loss: 3.4747 - val_acc_top_1: 0.6043
Epoch 00146: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 147/200
lr =  0.004000000000000001

Epoch 00147: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.5987 - acc_top_1: 0.9999 - val_loss: 2.5621 - val_acc_top_1: 0.5886
Epoch 00147: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 148/200
lr =  0.004000000000000001

Epoch 00148: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.5871 - acc_top_1: 0.9998 - val_loss: 2.9301 - val_acc_top_1: 0.6031
Epoch 00148: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 149/200
lr =  0.004000000000000001

Epoch 00149: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.5756 - acc_top_1: 0.9999 - val_loss: 2.2539 - val_acc_top_1: 0.6040
Epoch 00149: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 150/200
lr =  0.004000000000000001

Epoch 00150: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.5640 - acc_top_1: 0.9999 - val_loss: 2.6043 - val_acc_top_1: 0.5982
Epoch 00150: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 151/200
lr =  0.004000000000000001

Epoch 00151: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.5530 - acc_top_1: 0.9998 - val_loss: 2.6769 - val_acc_top_1: 0.5963
Epoch 00151: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 152/200
lr =  0.004000000000000001

Epoch 00152: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.5423 - acc_top_1: 0.9998 - val_loss: 2.4051 - val_acc_top_1: 0.5975
Epoch 00152: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 153/200
lr =  0.004000000000000001

Epoch 00153: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.5316 - acc_top_1: 0.9999 - val_loss: 2.5934 - val_acc_top_1: 0.5963
Epoch 00153: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 154/200
lr =  0.004000000000000001

Epoch 00154: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.5211 - acc_top_1: 0.9999 - val_loss: 2.6303 - val_acc_top_1: 0.5985
Epoch 00154: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 155/200
lr =  0.004000000000000001

Epoch 00155: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.5109 - acc_top_1: 0.9999 - val_loss: 2.6150 - val_acc_top_1: 0.5914
Epoch 00155: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 156/200
lr =  0.004000000000000001

Epoch 00156: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.5009 - acc_top_1: 0.9998 - val_loss: 3.2776 - val_acc_top_1: 0.5852
Epoch 00156: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 157/200
lr =  0.004000000000000001

Epoch 00157: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 199s - loss: 0.4915 - acc_top_1: 0.9998 - val_loss: 1.9394 - val_acc_top_1: 0.5874
Epoch 00157: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 158/200
lr =  0.004000000000000001

Epoch 00158: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.4815 - acc_top_1: 0.9999 - val_loss: 2.7218 - val_acc_top_1: 0.5822
Epoch 00158: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 159/200
lr =  0.004000000000000001

Epoch 00159: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.4723 - acc_top_1: 0.9998 - val_loss: 2.9707 - val_acc_top_1: 0.6028
Epoch 00159: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 160/200
lr =  0.004000000000000001

Epoch 00160: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 200s - loss: 0.4629 - acc_top_1: 0.9999 - val_loss: 2.0349 - val_acc_top_1: 0.6000
Epoch 00160: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 161/200
lr =  0.0008000000000000003

Epoch 00161: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 199s - loss: 0.4572 - acc_top_1: 0.9999 - val_loss: 2.5399 - val_acc_top_1: 0.6000
Epoch 00161: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 162/200
lr =  0.0008000000000000003

Epoch 00162: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 199s - loss: 0.4551 - acc_top_1: 0.9999 - val_loss: 2.7545 - val_acc_top_1: 0.5972
Epoch 00162: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 163/200
lr =  0.0008000000000000003

Epoch 00163: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4534 - acc_top_1: 0.9999 - val_loss: 2.0211 - val_acc_top_1: 0.6000
Epoch 00163: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 164/200
lr =  0.0008000000000000003

Epoch 00164: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4517 - acc_top_1: 0.9999 - val_loss: 1.8658 - val_acc_top_1: 0.5886
Epoch 00164: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 165/200
lr =  0.0008000000000000003

Epoch 00165: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4498 - acc_top_1: 0.9998 - val_loss: 2.7154 - val_acc_top_1: 0.6046
Epoch 00165: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 166/200
lr =  0.0008000000000000003

Epoch 00166: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4480 - acc_top_1: 0.9999 - val_loss: 2.4926 - val_acc_top_1: 0.6000
Epoch 00166: val_acc_top_1 did not improve from 0.60523

========================================================================

Epoch 167/200
lr =  0.0008000000000000003

Epoch 00167: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4462 - acc_top_1: 0.9999 - val_loss: 2.5811 - val_acc_top_1: 0.6055
Epoch 00167: val_acc_top_1 improved from 0.60523 to 0.60554, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_167.h5

========================================================================

Epoch 168/200
lr =  0.0008000000000000003

Epoch 00168: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4444 - acc_top_1: 0.9999 - val_loss: 1.7489 - val_acc_top_1: 0.6018
Epoch 00168: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 169/200
lr =  0.0008000000000000003

Epoch 00169: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4426 - acc_top_1: 0.9999 - val_loss: 3.0429 - val_acc_top_1: 0.6009
Epoch 00169: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 170/200
lr =  0.0008000000000000003

Epoch 00170: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 199s - loss: 0.4408 - acc_top_1: 0.9999 - val_loss: 2.2412 - val_acc_top_1: 0.5923
Epoch 00170: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 171/200
lr =  0.0008000000000000003

Epoch 00171: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4391 - acc_top_1: 0.9999 - val_loss: 1.9854 - val_acc_top_1: 0.5982
Epoch 00171: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 172/200
lr =  0.0008000000000000003

Epoch 00172: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 199s - loss: 0.4371 - acc_top_1: 1.0000 - val_loss: 1.8195 - val_acc_top_1: 0.5988
Epoch 00172: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 173/200
lr =  0.0008000000000000003

Epoch 00173: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4359 - acc_top_1: 0.9998 - val_loss: 2.7627 - val_acc_top_1: 0.5969
Epoch 00173: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 174/200
lr =  0.0008000000000000003

Epoch 00174: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4340 - acc_top_1: 0.9999 - val_loss: 3.2838 - val_acc_top_1: 0.6034
Epoch 00174: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 175/200
lr =  0.0008000000000000003

Epoch 00175: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4321 - acc_top_1: 0.9999 - val_loss: 1.8335 - val_acc_top_1: 0.5966
Epoch 00175: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 176/200
lr =  0.0008000000000000003

Epoch 00176: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4305 - acc_top_1: 0.9999 - val_loss: 2.3702 - val_acc_top_1: 0.6009
Epoch 00176: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 177/200
lr =  0.0008000000000000003

Epoch 00177: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4288 - acc_top_1: 0.9999 - val_loss: 2.4374 - val_acc_top_1: 0.5978
Epoch 00177: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 178/200
lr =  0.0008000000000000003

Epoch 00178: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4270 - acc_top_1: 0.9999 - val_loss: 2.3764 - val_acc_top_1: 0.6040
Epoch 00178: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 179/200
lr =  0.0008000000000000003

Epoch 00179: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4254 - acc_top_1: 0.9999 - val_loss: 2.4667 - val_acc_top_1: 0.6003
Epoch 00179: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 180/200
lr =  0.0008000000000000003

Epoch 00180: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4235 - acc_top_1: 1.0000 - val_loss: 2.6781 - val_acc_top_1: 0.5932
Epoch 00180: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 181/200
lr =  0.0008000000000000003

Epoch 00181: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4220 - acc_top_1: 0.9999 - val_loss: 2.6373 - val_acc_top_1: 0.5948
Epoch 00181: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 182/200
lr =  0.0008000000000000003

Epoch 00182: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 199s - loss: 0.4205 - acc_top_1: 0.9999 - val_loss: 1.9200 - val_acc_top_1: 0.6025
Epoch 00182: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 183/200
lr =  0.0008000000000000003

Epoch 00183: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4186 - acc_top_1: 1.0000 - val_loss: 1.9428 - val_acc_top_1: 0.5929
Epoch 00183: val_acc_top_1 did not improve from 0.60554

========================================================================

Epoch 184/200
lr =  0.0008000000000000003

Epoch 00184: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4172 - acc_top_1: 0.9999 - val_loss: 2.5581 - val_acc_top_1: 0.6058
Epoch 00184: val_acc_top_1 improved from 0.60554 to 0.60585, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/checkpoints/checkpoint_best_weights_184.h5

========================================================================

Epoch 185/200
lr =  0.0008000000000000003

Epoch 00185: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4153 - acc_top_1: 0.9999 - val_loss: 2.7008 - val_acc_top_1: 0.5883
Epoch 00185: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 186/200
lr =  0.0008000000000000003

Epoch 00186: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4136 - acc_top_1: 0.9999 - val_loss: 2.2328 - val_acc_top_1: 0.5969
Epoch 00186: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 187/200
lr =  0.0008000000000000003

Epoch 00187: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4121 - acc_top_1: 1.0000 - val_loss: 1.6265 - val_acc_top_1: 0.5911
Epoch 00187: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 188/200
lr =  0.0008000000000000003

Epoch 00188: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4105 - acc_top_1: 0.9999 - val_loss: 1.9086 - val_acc_top_1: 0.5997
Epoch 00188: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 189/200
lr =  0.0008000000000000003

Epoch 00189: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4088 - acc_top_1: 0.9999 - val_loss: 2.2512 - val_acc_top_1: 0.5985
Epoch 00189: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 190/200
lr =  0.0008000000000000003

Epoch 00190: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 199s - loss: 0.4073 - acc_top_1: 1.0000 - val_loss: 2.9545 - val_acc_top_1: 0.6028
Epoch 00190: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 191/200
lr =  0.0008000000000000003

Epoch 00191: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4056 - acc_top_1: 1.0000 - val_loss: 2.2151 - val_acc_top_1: 0.5960
Epoch 00191: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 192/200
lr =  0.0008000000000000003

Epoch 00192: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4041 - acc_top_1: 0.9998 - val_loss: 2.9175 - val_acc_top_1: 0.5972
Epoch 00192: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 193/200
lr =  0.0008000000000000003

Epoch 00193: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4024 - acc_top_1: 0.9999 - val_loss: 2.0094 - val_acc_top_1: 0.6028
Epoch 00193: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 194/200
lr =  0.0008000000000000003

Epoch 00194: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.4008 - acc_top_1: 0.9999 - val_loss: 2.4135 - val_acc_top_1: 0.6037
Epoch 00194: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 195/200
lr =  0.0008000000000000003

Epoch 00195: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.3993 - acc_top_1: 0.9999 - val_loss: 2.5967 - val_acc_top_1: 0.5905
Epoch 00195: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 196/200
lr =  0.0008000000000000003

Epoch 00196: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.3976 - acc_top_1: 1.0000 - val_loss: 2.7645 - val_acc_top_1: 0.5948
Epoch 00196: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 197/200
lr =  0.0008000000000000003

Epoch 00197: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.3961 - acc_top_1: 0.9999 - val_loss: 1.8011 - val_acc_top_1: 0.5954
Epoch 00197: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 198/200
lr =  0.0008000000000000003

Epoch 00198: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.3945 - acc_top_1: 1.0000 - val_loss: 2.6108 - val_acc_top_1: 0.6040
Epoch 00198: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 199/200
lr =  0.0008000000000000003

Epoch 00199: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.3929 - acc_top_1: 1.0000 - val_loss: 2.1376 - val_acc_top_1: 0.6012
Epoch 00199: val_acc_top_1 did not improve from 0.60585

========================================================================

Epoch 200/200
lr =  0.0008000000000000003

Epoch 00200: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 200s - loss: 0.3914 - acc_top_1: 0.9999 - val_loss: 2.3848 - val_acc_top_1: 0.5988
Epoch 00200: val_acc_top_1 did not improve from 0.60585

========================================================================

Start Time: 15:34:57 PM Friday 2020-08-28
Stop Time : 02:59:54 AM Saturday 2020-08-29
Run Time  : 11:24:57

Data Modifications:
  width_shift_range: 4.0
  height_shift_range: 4.0
  horizontal_flip: True
  featurewise_center: True
  featurewise_std_normalization: True

Notes:
  Expt Notes: Used to create src expts for tiny imagenet notliving vs living.

Peak Accuracy: 60.58% at epoch 184

Closing log results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/metadata
Saving log file to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt09.arl.army.mil_v0/metadata/Expt_output.log
