(dnn_py3) sgutstei@alt08 ~/Projects/opt-tfer> python run_expt.py ./cfg_dir/expt_cfg/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/src_net_1.cfg
CMD LINE ARGS:
config_files : ['./cfg_dir/expt_cfg/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/src_net_1.cfg']
gpu : 
dbg : False
epochs : 0
silent : False
nocheckpoint : False
Using TensorFlow backend.
Saving results to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0
Loading data from dataset_loaders/tinyimagenet200_src_notliving_vs_living
Encoding data
Creating 1-hot encodings -  hot/not_hot (1.0000 / 0.0000)
Creating encoding matrices for train & test data
Preprocessing Train Images
Preprocessing Test Images
Built Data Manager
2020-08-28 15:22:53.315187: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-08-28 15:22:53.968741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: Quadro M6000 computeCapability: 5.2
coreClock: 1.114GHz coreCount: 24 deviceMemorySize: 11.93GiB deviceMemoryBandwidth: 295.49GiB/s
2020-08-28 15:22:53.979264: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-28 15:22:54.379660: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-28 15:22:54.756645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-28 15:22:55.353168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-28 15:22:55.693155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-28 15:22:55.931988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-28 15:22:56.660777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-28 15:22:56.664457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-08-28 15:22:56.665134: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-08-28 15:22:56.685846: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3100120000 Hz
2020-08-28 15:22:56.690921: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556009254f60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-28 15:22:56.690983: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-28 15:22:56.692683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: Quadro M6000 computeCapability: 5.2
coreClock: 1.114GHz coreCount: 24 deviceMemorySize: 11.93GiB deviceMemoryBandwidth: 295.49GiB/s
2020-08-28 15:22:56.692779: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-28 15:22:56.692824: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-28 15:22:56.692864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-28 15:22:56.692910: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-28 15:22:56.692953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-28 15:22:56.693011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-28 15:22:56.693061: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-28 15:22:56.695854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-08-28 15:22:56.695941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-28 15:22:56.759146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-28 15:22:56.759222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-08-28 15:22:56.759249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-08-28 15:22:56.762997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11498 MB memory) -> physical GPU (device: 0, name: Quadro M6000, pci bus id: 0000:03:00.0, compute capability: 5.2)
2020-08-28 15:22:56.766408: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5560097454e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-28 15:22:56.766449: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro M6000, Compute Capability 5.2
Standard training
Initializing data manager ...
Using TensorFlow local_backend.
Initializing architecture ...
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(input)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (3, 3), padding="same", strides=(1, 1), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (1, 1), padding="same", strides=(1, 1), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:54: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (3, 3), padding="same", strides=(2, 2), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (1, 1), padding="same", strides=(2, 2), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:79: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:87: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(640, (3, 3), padding="same", strides=(2, 2), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(640, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(640, (1, 1), padding="same", strides=(2, 2), kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(init)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:104: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(640, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:112: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(640, (3, 3), padding="same", kernel_initializer="he_normal", use_bias=False, kernel_regularizer=<keras.reg...)`
  use_bias=False)(x)
/mnt/growler/barleyhome/sgutstei/Projects/opt-tfer/net_architectures/wide_residual_network_base.py:172: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(65, activation="softmax", kernel_regularizer=<keras.reg...)`
  x = Dense(nb_classes, W_regularizer=l2(weight_decay), activation='softmax')(x)
Wide Residual Network-28-10 created.
Starting with weights from epoch 0
Compiling model ...
Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.
Not saving graphical image of net

============================================================

Expt Info:

NB Epochs: 200
Expt Dir: results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0
Expt Prefix: alt08.arl.army.mil_v0

Model:
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   432         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  23040       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  230400      activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  2560        activation_1[0][0]               
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_3[0][0]                   
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         add_1[0][0]                      
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230400      activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230400      activation_4[0][0]               
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         add_2[0][0]                      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230400      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230400      activation_6[0][0]               
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         add_3[0][0]                      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230400      activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230400      activation_8[0][0]               
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 conv2d_10[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         add_4[0][0]                      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  460800      activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  921600      activation_10[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  51200       activation_9[0][0]               
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_12[0][0]                  
                                                                 conv2d_13[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        add_5[0][0]                      
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921600      activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921600      activation_12[0][0]              
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 conv2d_15[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        add_6[0][0]                      
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921600      activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921600      activation_14[0][0]              
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 conv2d_17[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        add_7[0][0]                      
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921600      activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921600      activation_16[0][0]              
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 conv2d_19[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        add_8[0][0]                      
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    1843200     activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    3686400     activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    204800      activation_17[0][0]              
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_21[0][0]                  
                                                                 conv2d_22[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        add_9[0][0]                      
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3686400     activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3686400     activation_20[0][0]              
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 conv2d_24[0][0]                  
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        add_10[0][0]                     
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3686400     activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3686400     activation_22[0][0]              
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 conv2d_26[0][0]                  
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        add_11[0][0]                     
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3686400     activation_23[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3686400     activation_24[0][0]              
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 conv2d_28[0][0]                  
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        add_12[0][0]                     
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 1, 1, 640)    0           activation_25[0][0]              
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 640)          0           average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 65)           41665       flatten_1[0][0]                  
==================================================================================================
Total params: 36,532,401
Trainable params: 36,514,449
Non-trainable params: 17,952
__________________________________________________________________________________________________

============================================================


3: +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  
7: -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  
8: -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  
9: -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
   -  -  -  -  -  
17: -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
18: -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
22: -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
24: -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
25: -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
26: -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
37: -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
38: -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
41: -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
42: -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
48: -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
49: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
50: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
54: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
58: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
62: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
64: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
68: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
69: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
73: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
76: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
77: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
82: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
87: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
88: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
89: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
91: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
96: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
98: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  
    -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
    -  -  -  -  -  
100: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
101: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
102: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
104: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
109: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
112: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
113: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
116: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
118: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
123: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
129: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
130: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
132: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
133: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
134: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
138: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
141: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
143: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
145: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  -  
     -  -  -  -  -  
148: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  -  
     -  -  -  -  -  
150: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  -  
     -  -  -  -  -  
152: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  -  
     -  -  -  -  -  
157: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  -  
     -  -  -  -  -  
160: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  -  
     -  -  -  -  -  
175: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  -  
     -  -  -  -  -  
178: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  -  
     -  -  -  -  -  
184: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  +  
     -  -  -  -  -  
186: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     +  -  -  -  -  
187: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  +  -  -  -  
191: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  +  -  -  
194: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  +  -  
197: -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  
     -  -  -  -  +  





Total Classes: 65

3:miniskirt -- 7:obelisk -- 8:plunger -- 9:bucket -- 17:cannon -- 
18:broom -- 22:chain -- 24:espresso -- 25:seashore -- 26:organ -- 
37:torch -- 38:tractor -- 41:desk -- 42:beacon -- 48:reel -- 
49:iPod -- 50:abacus -- 54:dam -- 58:cliff -- 62:teapot -- 
64:sunglasses -- 68:maypole -- 69:jinrikisha -- 73:barn -- 76:snorkel -- 
77:turnstile -- 82:apron -- 87:potpie -- 88:lampshade -- 89:volleyball -- 
91:lifeboat -- 96:crane -- 98:pretzel -- 100:oboe -- 101:syringe -- 
102:birdhouse -- 104:fountain -- 109:barrel -- 112:alp -- 113:lakeside -- 
116:vestment -- 118:gasmask -- 123:flagpole -- 129:wok -- 130:sandal -- 
132:beaker -- 133:candle -- 134:basketball -- 138:nail -- 141:thatch -- 
143:hourglass -- 145:plate -- 148:refrigerator -- 150:trolleybus -- 152:umbrella -- 
157:dumbbell -- 160:sock -- 175:backpack -- 178:cardigan -- 184:pizza -- 
186:limousine -- 187:stopwatch -- 191:chest -- 194:sombrero -- 197:projectile -- 

Built Net Manager
QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-sgutstei'
Qt: XKEYBOARD extension not present on the X server.
2020-08-28 15:23:11.325754: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-28 15:23:11.747136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-28 15:23:15.366747: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.

Init loss and acc:                             loss:  14.33526 - acc_top_1: 0.01385 - val_loss: 14.33526 - val_acc_top_1: 0.01323
Saving results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_encodings_0.pkl
Epoch 00000: val_acc_top_1 improved from -inf to 0.01323, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_0.h5

========================================================================

Epoch 1/200
lr =  0.1

Epoch 00001: LearningRateScheduler setting learning rate to 0.1.
 - 209s - loss: 11.9335 - acc_top_1: 0.0701 - val_loss: 9.6862 - val_acc_top_1: 0.1218
Epoch 00001: val_acc_top_1 improved from 0.01323 to 0.12185, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_1.h5

========================================================================

Epoch 2/200
lr =  0.1

Epoch 00002: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 8.2295 - acc_top_1: 0.1635 - val_loss: 7.1278 - val_acc_top_1: 0.2111
Epoch 00002: val_acc_top_1 improved from 0.12185 to 0.21108, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_2.h5

========================================================================

Epoch 3/200
lr =  0.1

Epoch 00003: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 6.0476 - acc_top_1: 0.2285 - val_loss: 5.3054 - val_acc_top_1: 0.2425
Epoch 00003: val_acc_top_1 improved from 0.21108 to 0.24246, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_3.h5

========================================================================

Epoch 4/200
lr =  0.1

Epoch 00004: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 4.7457 - acc_top_1: 0.2798 - val_loss: 3.9703 - val_acc_top_1: 0.3034
Epoch 00004: val_acc_top_1 improved from 0.24246 to 0.30338, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_4.h5

========================================================================

Epoch 5/200
lr =  0.1

Epoch 00005: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 3.9588 - acc_top_1: 0.3197 - val_loss: 3.7205 - val_acc_top_1: 0.3335
Epoch 00005: val_acc_top_1 improved from 0.30338 to 0.33354, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_5.h5

========================================================================

Epoch 6/200
lr =  0.1

Epoch 00006: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 3.4652 - acc_top_1: 0.3565 - val_loss: 3.6561 - val_acc_top_1: 0.3498
Epoch 00006: val_acc_top_1 improved from 0.33354 to 0.34985, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_6.h5

========================================================================

Epoch 7/200
lr =  0.1

Epoch 00007: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 3.1651 - acc_top_1: 0.3793 - val_loss: 3.4722 - val_acc_top_1: 0.3895
Epoch 00007: val_acc_top_1 improved from 0.34985 to 0.38954, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_7.h5

========================================================================

Epoch 8/200
lr =  0.1

Epoch 00008: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.9631 - acc_top_1: 0.4113 - val_loss: 3.0077 - val_acc_top_1: 0.4095
Epoch 00008: val_acc_top_1 improved from 0.38954 to 0.40954, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_8.h5

========================================================================

Epoch 9/200
lr =  0.1

Epoch 00009: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.8410 - acc_top_1: 0.4282 - val_loss: 2.5215 - val_acc_top_1: 0.4142
Epoch 00009: val_acc_top_1 improved from 0.40954 to 0.41415, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_9.h5

========================================================================

Epoch 10/200
lr =  0.1

Epoch 00010: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.7601 - acc_top_1: 0.4500 - val_loss: 2.7969 - val_acc_top_1: 0.4449
Epoch 00010: val_acc_top_1 improved from 0.41415 to 0.44492, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_10.h5

========================================================================

Epoch 11/200
lr =  0.1

Epoch 00011: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7149 - acc_top_1: 0.4658 - val_loss: 3.0169 - val_acc_top_1: 0.4523
Epoch 00011: val_acc_top_1 improved from 0.44492 to 0.45231, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_11.h5

========================================================================

Epoch 12/200
lr =  0.1

Epoch 00012: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.6944 - acc_top_1: 0.4754 - val_loss: 2.9746 - val_acc_top_1: 0.4462
Epoch 00012: val_acc_top_1 did not improve from 0.45231

========================================================================

Epoch 13/200
lr =  0.1

Epoch 00013: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.6597 - acc_top_1: 0.4901 - val_loss: 3.2063 - val_acc_top_1: 0.4763
Epoch 00013: val_acc_top_1 improved from 0.45231 to 0.47631, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_13.h5

========================================================================

Epoch 14/200
lr =  0.1

Epoch 00014: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.6526 - acc_top_1: 0.5007 - val_loss: 3.0170 - val_acc_top_1: 0.4572
Epoch 00014: val_acc_top_1 did not improve from 0.47631

========================================================================

Epoch 15/200
lr =  0.1

Epoch 00015: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.6468 - acc_top_1: 0.5056 - val_loss: 3.2187 - val_acc_top_1: 0.4705
Epoch 00015: val_acc_top_1 did not improve from 0.47631

========================================================================

Epoch 16/200
lr =  0.1

Epoch 00016: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.6326 - acc_top_1: 0.5191 - val_loss: 2.4276 - val_acc_top_1: 0.4800
Epoch 00016: val_acc_top_1 improved from 0.47631 to 0.48000, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_16.h5

========================================================================

Epoch 17/200
lr =  0.1

Epoch 00017: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.6409 - acc_top_1: 0.5229 - val_loss: 2.5703 - val_acc_top_1: 0.4674
Epoch 00017: val_acc_top_1 did not improve from 0.48000

========================================================================

Epoch 18/200
lr =  0.1

Epoch 00018: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.6457 - acc_top_1: 0.5306 - val_loss: 2.9443 - val_acc_top_1: 0.4902
Epoch 00018: val_acc_top_1 improved from 0.48000 to 0.49015, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_18.h5

========================================================================

Epoch 19/200
lr =  0.1

Epoch 00019: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.6363 - acc_top_1: 0.5389 - val_loss: 3.1848 - val_acc_top_1: 0.4908
Epoch 00019: val_acc_top_1 improved from 0.49015 to 0.49077, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_19.h5

========================================================================

Epoch 20/200
lr =  0.1

Epoch 00020: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.6378 - acc_top_1: 0.5478 - val_loss: 3.2308 - val_acc_top_1: 0.4985
Epoch 00020: val_acc_top_1 improved from 0.49077 to 0.49846, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_20.h5

========================================================================

Epoch 21/200
lr =  0.1

Epoch 00021: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.6505 - acc_top_1: 0.5482 - val_loss: 2.8763 - val_acc_top_1: 0.4932
Epoch 00021: val_acc_top_1 did not improve from 0.49846

========================================================================

Epoch 22/200
lr =  0.1

Epoch 00022: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.6498 - acc_top_1: 0.5576 - val_loss: 3.2804 - val_acc_top_1: 0.4874
Epoch 00022: val_acc_top_1 did not improve from 0.49846

========================================================================

Epoch 23/200
lr =  0.1

Epoch 00023: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.6557 - acc_top_1: 0.5591 - val_loss: 2.8323 - val_acc_top_1: 0.5018
Epoch 00023: val_acc_top_1 improved from 0.49846 to 0.50185, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_23.h5

========================================================================

Epoch 24/200
lr =  0.1

Epoch 00024: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.6648 - acc_top_1: 0.5661 - val_loss: 2.9057 - val_acc_top_1: 0.4957
Epoch 00024: val_acc_top_1 did not improve from 0.50185

========================================================================

Epoch 25/200
lr =  0.1

Epoch 00025: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.6696 - acc_top_1: 0.5669 - val_loss: 3.0531 - val_acc_top_1: 0.4858
Epoch 00025: val_acc_top_1 did not improve from 0.50185

========================================================================

Epoch 26/200
lr =  0.1

Epoch 00026: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.6803 - acc_top_1: 0.5695 - val_loss: 3.0361 - val_acc_top_1: 0.5031
Epoch 00026: val_acc_top_1 improved from 0.50185 to 0.50308, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_26.h5

========================================================================

Epoch 27/200
lr =  0.1

Epoch 00027: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.6710 - acc_top_1: 0.5770 - val_loss: 2.6899 - val_acc_top_1: 0.4991
Epoch 00027: val_acc_top_1 did not improve from 0.50308

========================================================================

Epoch 28/200
lr =  0.1

Epoch 00028: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.6799 - acc_top_1: 0.5776 - val_loss: 2.7538 - val_acc_top_1: 0.5185
Epoch 00028: val_acc_top_1 improved from 0.50308 to 0.51846, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_28.h5

========================================================================

Epoch 29/200
lr =  0.1

Epoch 00029: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.6965 - acc_top_1: 0.5804 - val_loss: 3.2379 - val_acc_top_1: 0.5055
Epoch 00029: val_acc_top_1 did not improve from 0.51846

========================================================================

Epoch 30/200
lr =  0.1

Epoch 00030: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.6920 - acc_top_1: 0.5853 - val_loss: 3.1808 - val_acc_top_1: 0.5222
Epoch 00030: val_acc_top_1 improved from 0.51846 to 0.52215, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_30.h5

========================================================================

Epoch 31/200
lr =  0.1

Epoch 00031: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.7013 - acc_top_1: 0.5872 - val_loss: 3.7019 - val_acc_top_1: 0.4994
Epoch 00031: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 32/200
lr =  0.1

Epoch 00032: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7077 - acc_top_1: 0.5892 - val_loss: 3.1740 - val_acc_top_1: 0.5028
Epoch 00032: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 33/200
lr =  0.1

Epoch 00033: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7042 - acc_top_1: 0.5951 - val_loss: 3.1234 - val_acc_top_1: 0.5040
Epoch 00033: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 34/200
lr =  0.1

Epoch 00034: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7228 - acc_top_1: 0.5934 - val_loss: 3.2168 - val_acc_top_1: 0.5203
Epoch 00034: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 35/200
lr =  0.1

Epoch 00035: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7231 - acc_top_1: 0.6003 - val_loss: 3.0034 - val_acc_top_1: 0.5172
Epoch 00035: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 36/200
lr =  0.1

Epoch 00036: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7228 - acc_top_1: 0.6027 - val_loss: 3.5586 - val_acc_top_1: 0.5003
Epoch 00036: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 37/200
lr =  0.1

Epoch 00037: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.7325 - acc_top_1: 0.6006 - val_loss: 3.2479 - val_acc_top_1: 0.5028
Epoch 00037: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 38/200
lr =  0.1

Epoch 00038: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7492 - acc_top_1: 0.6041 - val_loss: 3.1005 - val_acc_top_1: 0.5129
Epoch 00038: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 39/200
lr =  0.1

Epoch 00039: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7490 - acc_top_1: 0.6053 - val_loss: 3.8300 - val_acc_top_1: 0.5000
Epoch 00039: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 40/200
lr =  0.1

Epoch 00040: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.7404 - acc_top_1: 0.6091 - val_loss: 3.2935 - val_acc_top_1: 0.5089
Epoch 00040: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 41/200
lr =  0.1

Epoch 00041: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7590 - acc_top_1: 0.6065 - val_loss: 2.7835 - val_acc_top_1: 0.5175
Epoch 00041: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 42/200
lr =  0.1

Epoch 00042: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.7560 - acc_top_1: 0.6112 - val_loss: 3.4254 - val_acc_top_1: 0.5071
Epoch 00042: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 43/200
lr =  0.1

Epoch 00043: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7552 - acc_top_1: 0.6193 - val_loss: 3.4011 - val_acc_top_1: 0.5185
Epoch 00043: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 44/200
lr =  0.1

Epoch 00044: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7779 - acc_top_1: 0.6123 - val_loss: 3.3606 - val_acc_top_1: 0.5080
Epoch 00044: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 45/200
lr =  0.1

Epoch 00045: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7788 - acc_top_1: 0.6164 - val_loss: 3.4032 - val_acc_top_1: 0.5095
Epoch 00045: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 46/200
lr =  0.1

Epoch 00046: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7717 - acc_top_1: 0.6199 - val_loss: 3.5219 - val_acc_top_1: 0.5117
Epoch 00046: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 47/200
lr =  0.1

Epoch 00047: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7857 - acc_top_1: 0.6221 - val_loss: 3.3753 - val_acc_top_1: 0.5028
Epoch 00047: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 48/200
lr =  0.1

Epoch 00048: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7915 - acc_top_1: 0.6219 - val_loss: 3.1900 - val_acc_top_1: 0.5172
Epoch 00048: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 49/200
lr =  0.1

Epoch 00049: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.7842 - acc_top_1: 0.6234 - val_loss: 2.8903 - val_acc_top_1: 0.5191
Epoch 00049: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 50/200
lr =  0.1

Epoch 00050: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.8060 - acc_top_1: 0.6250 - val_loss: 3.7767 - val_acc_top_1: 0.5154
Epoch 00050: val_acc_top_1 did not improve from 0.52215

========================================================================

Epoch 51/200
lr =  0.1

Epoch 00051: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.8153 - acc_top_1: 0.6231 - val_loss: 3.1358 - val_acc_top_1: 0.5317
Epoch 00051: val_acc_top_1 improved from 0.52215 to 0.53169, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_51.h5

========================================================================

Epoch 52/200
lr =  0.1

Epoch 00052: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.8078 - acc_top_1: 0.6311 - val_loss: 3.2513 - val_acc_top_1: 0.5126
Epoch 00052: val_acc_top_1 did not improve from 0.53169

========================================================================

Epoch 53/200
lr =  0.1

Epoch 00053: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.8078 - acc_top_1: 0.6299 - val_loss: 3.9203 - val_acc_top_1: 0.5160
Epoch 00053: val_acc_top_1 did not improve from 0.53169

========================================================================

Epoch 54/200
lr =  0.1

Epoch 00054: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.8299 - acc_top_1: 0.6238 - val_loss: 3.2775 - val_acc_top_1: 0.5154
Epoch 00054: val_acc_top_1 did not improve from 0.53169

========================================================================

Epoch 55/200
lr =  0.1

Epoch 00055: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.8134 - acc_top_1: 0.6322 - val_loss: 3.4666 - val_acc_top_1: 0.5049
Epoch 00055: val_acc_top_1 did not improve from 0.53169

========================================================================

Epoch 56/200
lr =  0.1

Epoch 00056: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.8410 - acc_top_1: 0.6278 - val_loss: 3.2045 - val_acc_top_1: 0.5120
Epoch 00056: val_acc_top_1 did not improve from 0.53169

========================================================================

Epoch 57/200
lr =  0.1

Epoch 00057: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.8135 - acc_top_1: 0.6321 - val_loss: 3.5787 - val_acc_top_1: 0.5095
Epoch 00057: val_acc_top_1 did not improve from 0.53169

========================================================================

Epoch 58/200
lr =  0.1

Epoch 00058: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.8291 - acc_top_1: 0.6344 - val_loss: 3.6970 - val_acc_top_1: 0.5175
Epoch 00058: val_acc_top_1 did not improve from 0.53169

========================================================================

Epoch 59/200
lr =  0.1

Epoch 00059: LearningRateScheduler setting learning rate to 0.1.
 - 202s - loss: 2.8240 - acc_top_1: 0.6361 - val_loss: 3.5273 - val_acc_top_1: 0.5065
Epoch 00059: val_acc_top_1 did not improve from 0.53169

========================================================================

Epoch 60/200
lr =  0.1

Epoch 00060: LearningRateScheduler setting learning rate to 0.1.
 - 201s - loss: 2.8457 - acc_top_1: 0.6331 - val_loss: 3.3757 - val_acc_top_1: 0.5046
Epoch 00060: val_acc_top_1 did not improve from 0.53169

========================================================================

Epoch 61/200
lr =  0.020000000000000004

Epoch 00061: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 2.3091 - acc_top_1: 0.7730 - val_loss: 2.7571 - val_acc_top_1: 0.6034
Epoch 00061: val_acc_top_1 improved from 0.53169 to 0.60338, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_61.h5

========================================================================

Epoch 62/200
lr =  0.020000000000000004

Epoch 00062: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.8966 - acc_top_1: 0.8566 - val_loss: 2.6978 - val_acc_top_1: 0.6074
Epoch 00062: val_acc_top_1 improved from 0.60338 to 0.60738, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_62.h5

========================================================================

Epoch 63/200
lr =  0.020000000000000004

Epoch 00063: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.6672 - acc_top_1: 0.8936 - val_loss: 2.9007 - val_acc_top_1: 0.6018
Epoch 00063: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 64/200
lr =  0.020000000000000004

Epoch 00064: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.5102 - acc_top_1: 0.9137 - val_loss: 3.0323 - val_acc_top_1: 0.5905
Epoch 00064: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 65/200
lr =  0.020000000000000004

Epoch 00065: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.3871 - acc_top_1: 0.9270 - val_loss: 2.5696 - val_acc_top_1: 0.5806
Epoch 00065: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 66/200
lr =  0.020000000000000004

Epoch 00066: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.2960 - acc_top_1: 0.9341 - val_loss: 3.4865 - val_acc_top_1: 0.5809
Epoch 00066: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 67/200
lr =  0.020000000000000004

Epoch 00067: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.2347 - acc_top_1: 0.9357 - val_loss: 2.9970 - val_acc_top_1: 0.5772
Epoch 00067: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 68/200
lr =  0.020000000000000004

Epoch 00068: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1884 - acc_top_1: 0.9357 - val_loss: 3.0170 - val_acc_top_1: 0.5809
Epoch 00068: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 69/200
lr =  0.020000000000000004

Epoch 00069: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1725 - acc_top_1: 0.9321 - val_loss: 2.6951 - val_acc_top_1: 0.5612
Epoch 00069: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 70/200
lr =  0.020000000000000004

Epoch 00070: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 202s - loss: 1.1698 - acc_top_1: 0.9249 - val_loss: 2.5245 - val_acc_top_1: 0.5714
Epoch 00070: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 71/200
lr =  0.020000000000000004

Epoch 00071: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 202s - loss: 1.1591 - acc_top_1: 0.9264 - val_loss: 2.0597 - val_acc_top_1: 0.5705
Epoch 00071: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 72/200
lr =  0.020000000000000004

Epoch 00072: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1500 - acc_top_1: 0.9265 - val_loss: 3.5694 - val_acc_top_1: 0.5548
Epoch 00072: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 73/200
lr =  0.020000000000000004

Epoch 00073: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1694 - acc_top_1: 0.9190 - val_loss: 2.3946 - val_acc_top_1: 0.5538
Epoch 00073: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 74/200
lr =  0.020000000000000004

Epoch 00074: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1540 - acc_top_1: 0.9253 - val_loss: 3.0223 - val_acc_top_1: 0.5689
Epoch 00074: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 75/200
lr =  0.020000000000000004

Epoch 00075: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1379 - acc_top_1: 0.9288 - val_loss: 3.1938 - val_acc_top_1: 0.5508
Epoch 00075: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 76/200
lr =  0.020000000000000004

Epoch 00076: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1586 - acc_top_1: 0.9217 - val_loss: 3.2002 - val_acc_top_1: 0.5502
Epoch 00076: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 77/200
lr =  0.020000000000000004

Epoch 00077: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1495 - acc_top_1: 0.9282 - val_loss: 3.1522 - val_acc_top_1: 0.5538
Epoch 00077: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 78/200
lr =  0.020000000000000004

Epoch 00078: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1772 - acc_top_1: 0.9182 - val_loss: 2.9765 - val_acc_top_1: 0.5615
Epoch 00078: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 79/200
lr =  0.020000000000000004

Epoch 00079: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1547 - acc_top_1: 0.9277 - val_loss: 2.7818 - val_acc_top_1: 0.5603
Epoch 00079: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 80/200
lr =  0.020000000000000004

Epoch 00080: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1639 - acc_top_1: 0.9255 - val_loss: 3.8587 - val_acc_top_1: 0.5532
Epoch 00080: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 81/200
lr =  0.020000000000000004

Epoch 00081: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1468 - acc_top_1: 0.9319 - val_loss: 3.1661 - val_acc_top_1: 0.5542
Epoch 00081: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 82/200
lr =  0.020000000000000004

Epoch 00082: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1520 - acc_top_1: 0.9272 - val_loss: 2.5335 - val_acc_top_1: 0.5514
Epoch 00082: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 83/200
lr =  0.020000000000000004

Epoch 00083: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1531 - acc_top_1: 0.9291 - val_loss: 2.3500 - val_acc_top_1: 0.5551
Epoch 00083: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 84/200
lr =  0.020000000000000004

Epoch 00084: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1684 - acc_top_1: 0.9260 - val_loss: 2.9886 - val_acc_top_1: 0.5535
Epoch 00084: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 85/200
lr =  0.020000000000000004

Epoch 00085: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1707 - acc_top_1: 0.9276 - val_loss: 2.7723 - val_acc_top_1: 0.5505
Epoch 00085: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 86/200
lr =  0.020000000000000004

Epoch 00086: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1798 - acc_top_1: 0.9285 - val_loss: 3.3018 - val_acc_top_1: 0.5603
Epoch 00086: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 87/200
lr =  0.020000000000000004

Epoch 00087: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1604 - acc_top_1: 0.9343 - val_loss: 2.7135 - val_acc_top_1: 0.5526
Epoch 00087: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 88/200
lr =  0.020000000000000004

Epoch 00088: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1508 - acc_top_1: 0.9340 - val_loss: 2.7448 - val_acc_top_1: 0.5535
Epoch 00088: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 89/200
lr =  0.020000000000000004

Epoch 00089: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1925 - acc_top_1: 0.9231 - val_loss: 3.2116 - val_acc_top_1: 0.5495
Epoch 00089: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 90/200
lr =  0.020000000000000004

Epoch 00090: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1806 - acc_top_1: 0.9306 - val_loss: 3.1876 - val_acc_top_1: 0.5462
Epoch 00090: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 91/200
lr =  0.020000000000000004

Epoch 00091: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1698 - acc_top_1: 0.9326 - val_loss: 2.3126 - val_acc_top_1: 0.5483
Epoch 00091: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 92/200
lr =  0.020000000000000004

Epoch 00092: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1542 - acc_top_1: 0.9370 - val_loss: 3.4428 - val_acc_top_1: 0.5551
Epoch 00092: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 93/200
lr =  0.020000000000000004

Epoch 00093: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1751 - acc_top_1: 0.9310 - val_loss: 3.1436 - val_acc_top_1: 0.5495
Epoch 00093: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 94/200
lr =  0.020000000000000004

Epoch 00094: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1671 - acc_top_1: 0.9348 - val_loss: 3.7044 - val_acc_top_1: 0.5437
Epoch 00094: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 95/200
lr =  0.020000000000000004

Epoch 00095: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1910 - acc_top_1: 0.9297 - val_loss: 3.0528 - val_acc_top_1: 0.5569
Epoch 00095: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 96/200
lr =  0.020000000000000004

Epoch 00096: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1824 - acc_top_1: 0.9328 - val_loss: 2.7290 - val_acc_top_1: 0.5532
Epoch 00096: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 97/200
lr =  0.020000000000000004

Epoch 00097: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1611 - acc_top_1: 0.9389 - val_loss: 2.7108 - val_acc_top_1: 0.5477
Epoch 00097: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 98/200
lr =  0.020000000000000004

Epoch 00098: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1641 - acc_top_1: 0.9374 - val_loss: 3.0928 - val_acc_top_1: 0.5440
Epoch 00098: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 99/200
lr =  0.020000000000000004

Epoch 00099: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 200s - loss: 1.1690 - acc_top_1: 0.9370 - val_loss: 2.8162 - val_acc_top_1: 0.5366
Epoch 00099: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 100/200
lr =  0.020000000000000004

Epoch 00100: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1901 - acc_top_1: 0.9321 - val_loss: 3.6716 - val_acc_top_1: 0.5498
Epoch 00100: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 101/200
lr =  0.020000000000000004

Epoch 00101: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1852 - acc_top_1: 0.9330 - val_loss: 2.8130 - val_acc_top_1: 0.5511
Epoch 00101: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 102/200
lr =  0.020000000000000004

Epoch 00102: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1992 - acc_top_1: 0.9315 - val_loss: 2.6878 - val_acc_top_1: 0.5489
Epoch 00102: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 103/200
lr =  0.020000000000000004

Epoch 00103: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1914 - acc_top_1: 0.9338 - val_loss: 2.7721 - val_acc_top_1: 0.5231
Epoch 00103: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 104/200
lr =  0.020000000000000004

Epoch 00104: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.2084 - acc_top_1: 0.9325 - val_loss: 3.0626 - val_acc_top_1: 0.5311
Epoch 00104: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 105/200
lr =  0.020000000000000004

Epoch 00105: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1932 - acc_top_1: 0.9363 - val_loss: 3.1940 - val_acc_top_1: 0.5348
Epoch 00105: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 106/200
lr =  0.020000000000000004

Epoch 00106: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1842 - acc_top_1: 0.9390 - val_loss: 3.8185 - val_acc_top_1: 0.5363
Epoch 00106: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 107/200
lr =  0.020000000000000004

Epoch 00107: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1898 - acc_top_1: 0.9370 - val_loss: 3.1762 - val_acc_top_1: 0.5391
Epoch 00107: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 108/200
lr =  0.020000000000000004

Epoch 00108: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1878 - acc_top_1: 0.9363 - val_loss: 3.8277 - val_acc_top_1: 0.5415
Epoch 00108: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 109/200
lr =  0.020000000000000004

Epoch 00109: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1935 - acc_top_1: 0.9365 - val_loss: 3.6170 - val_acc_top_1: 0.5360
Epoch 00109: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 110/200
lr =  0.020000000000000004

Epoch 00110: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.2213 - acc_top_1: 0.9289 - val_loss: 2.9885 - val_acc_top_1: 0.5332
Epoch 00110: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 111/200
lr =  0.020000000000000004

Epoch 00111: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1827 - acc_top_1: 0.9415 - val_loss: 2.7307 - val_acc_top_1: 0.5428
Epoch 00111: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 112/200
lr =  0.020000000000000004

Epoch 00112: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1955 - acc_top_1: 0.9363 - val_loss: 3.9787 - val_acc_top_1: 0.5385
Epoch 00112: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 113/200
lr =  0.020000000000000004

Epoch 00113: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.2055 - acc_top_1: 0.9343 - val_loss: 3.6463 - val_acc_top_1: 0.5403
Epoch 00113: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 114/200
lr =  0.020000000000000004

Epoch 00114: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.2218 - acc_top_1: 0.9301 - val_loss: 2.9722 - val_acc_top_1: 0.5406
Epoch 00114: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 115/200
lr =  0.020000000000000004

Epoch 00115: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1967 - acc_top_1: 0.9387 - val_loss: 3.4443 - val_acc_top_1: 0.5422
Epoch 00115: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 116/200
lr =  0.020000000000000004

Epoch 00116: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.1897 - acc_top_1: 0.9381 - val_loss: 3.5926 - val_acc_top_1: 0.5357
Epoch 00116: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 117/200
lr =  0.020000000000000004

Epoch 00117: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.2293 - acc_top_1: 0.9293 - val_loss: 3.1985 - val_acc_top_1: 0.5345
Epoch 00117: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 118/200
lr =  0.020000000000000004

Epoch 00118: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.2014 - acc_top_1: 0.9386 - val_loss: 3.1552 - val_acc_top_1: 0.5585
Epoch 00118: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 119/200
lr =  0.020000000000000004

Epoch 00119: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.2169 - acc_top_1: 0.9335 - val_loss: 3.2913 - val_acc_top_1: 0.5486
Epoch 00119: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 120/200
lr =  0.020000000000000004

Epoch 00120: LearningRateScheduler setting learning rate to 0.020000000000000004.
 - 201s - loss: 1.2268 - acc_top_1: 0.9323 - val_loss: 2.8682 - val_acc_top_1: 0.5489
Epoch 00120: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 121/200
lr =  0.004000000000000001

Epoch 00121: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 1.0948 - acc_top_1: 0.9750 - val_loss: 3.0246 - val_acc_top_1: 0.5871
Epoch 00121: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 122/200
lr =  0.004000000000000001

Epoch 00122: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 1.0121 - acc_top_1: 0.9938 - val_loss: 3.2936 - val_acc_top_1: 0.5812
Epoch 00122: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 123/200
lr =  0.004000000000000001

Epoch 00123: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.9824 - acc_top_1: 0.9967 - val_loss: 3.1058 - val_acc_top_1: 0.5935
Epoch 00123: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 124/200
lr =  0.004000000000000001

Epoch 00124: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.9593 - acc_top_1: 0.9978 - val_loss: 2.9503 - val_acc_top_1: 0.5886
Epoch 00124: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 125/200
lr =  0.004000000000000001

Epoch 00125: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.9381 - acc_top_1: 0.9986 - val_loss: 2.8029 - val_acc_top_1: 0.5963
Epoch 00125: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 126/200
lr =  0.004000000000000001

Epoch 00126: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 202s - loss: 0.9175 - acc_top_1: 0.9987 - val_loss: 2.9770 - val_acc_top_1: 0.5917
Epoch 00126: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 127/200
lr =  0.004000000000000001

Epoch 00127: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 202s - loss: 0.8988 - acc_top_1: 0.9989 - val_loss: 3.1901 - val_acc_top_1: 0.5929
Epoch 00127: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 128/200
lr =  0.004000000000000001

Epoch 00128: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 202s - loss: 0.8806 - acc_top_1: 0.9993 - val_loss: 3.6703 - val_acc_top_1: 0.5877
Epoch 00128: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 129/200
lr =  0.004000000000000001

Epoch 00129: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.8622 - acc_top_1: 0.9995 - val_loss: 2.6049 - val_acc_top_1: 0.5883
Epoch 00129: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 130/200
lr =  0.004000000000000001

Epoch 00130: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.8445 - acc_top_1: 0.9995 - val_loss: 3.5034 - val_acc_top_1: 0.6000
Epoch 00130: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 131/200
lr =  0.004000000000000001

Epoch 00131: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.8276 - acc_top_1: 0.9995 - val_loss: 3.7117 - val_acc_top_1: 0.5997
Epoch 00131: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 132/200
lr =  0.004000000000000001

Epoch 00132: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.8112 - acc_top_1: 0.9995 - val_loss: 2.2880 - val_acc_top_1: 0.5972
Epoch 00132: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 133/200
lr =  0.004000000000000001

Epoch 00133: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.7951 - acc_top_1: 0.9995 - val_loss: 2.3866 - val_acc_top_1: 0.5991
Epoch 00133: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 134/200
lr =  0.004000000000000001

Epoch 00134: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.7788 - acc_top_1: 0.9998 - val_loss: 2.3412 - val_acc_top_1: 0.6003
Epoch 00134: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 135/200
lr =  0.004000000000000001

Epoch 00135: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.7636 - acc_top_1: 0.9996 - val_loss: 3.2134 - val_acc_top_1: 0.6062
Epoch 00135: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 136/200
lr =  0.004000000000000001

Epoch 00136: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.7482 - acc_top_1: 0.9998 - val_loss: 2.8569 - val_acc_top_1: 0.5862
Epoch 00136: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 137/200
lr =  0.004000000000000001

Epoch 00137: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.7338 - acc_top_1: 0.9997 - val_loss: 2.8890 - val_acc_top_1: 0.6025
Epoch 00137: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 138/200
lr =  0.004000000000000001

Epoch 00138: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 202s - loss: 0.7189 - acc_top_1: 0.9998 - val_loss: 2.1274 - val_acc_top_1: 0.6000
Epoch 00138: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 139/200
lr =  0.004000000000000001

Epoch 00139: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.7051 - acc_top_1: 0.9997 - val_loss: 2.5680 - val_acc_top_1: 0.5982
Epoch 00139: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 140/200
lr =  0.004000000000000001

Epoch 00140: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.6908 - acc_top_1: 0.9998 - val_loss: 2.9282 - val_acc_top_1: 0.5997
Epoch 00140: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 141/200
lr =  0.004000000000000001

Epoch 00141: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.6771 - acc_top_1: 0.9998 - val_loss: 2.7182 - val_acc_top_1: 0.5982
Epoch 00141: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 142/200
lr =  0.004000000000000001

Epoch 00142: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.6641 - acc_top_1: 0.9997 - val_loss: 2.4120 - val_acc_top_1: 0.6055
Epoch 00142: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 143/200
lr =  0.004000000000000001

Epoch 00143: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.6504 - acc_top_1: 0.9998 - val_loss: 2.1316 - val_acc_top_1: 0.6031
Epoch 00143: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 144/200
lr =  0.004000000000000001

Epoch 00144: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.6377 - acc_top_1: 0.9999 - val_loss: 2.1192 - val_acc_top_1: 0.6028
Epoch 00144: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 145/200
lr =  0.004000000000000001

Epoch 00145: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.6250 - acc_top_1: 0.9999 - val_loss: 2.6985 - val_acc_top_1: 0.5948
Epoch 00145: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 146/200
lr =  0.004000000000000001

Epoch 00146: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 202s - loss: 0.6126 - acc_top_1: 0.9998 - val_loss: 1.7824 - val_acc_top_1: 0.6025
Epoch 00146: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 147/200
lr =  0.004000000000000001

Epoch 00147: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 202s - loss: 0.6007 - acc_top_1: 0.9998 - val_loss: 2.4926 - val_acc_top_1: 0.6062
Epoch 00147: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 148/200
lr =  0.004000000000000001

Epoch 00148: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.5889 - acc_top_1: 0.9999 - val_loss: 2.7349 - val_acc_top_1: 0.5923
Epoch 00148: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 149/200
lr =  0.004000000000000001

Epoch 00149: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.5773 - acc_top_1: 0.9999 - val_loss: 2.3080 - val_acc_top_1: 0.6018
Epoch 00149: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 150/200
lr =  0.004000000000000001

Epoch 00150: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 202s - loss: 0.5657 - acc_top_1: 1.0000 - val_loss: 2.2797 - val_acc_top_1: 0.5969
Epoch 00150: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 151/200
lr =  0.004000000000000001

Epoch 00151: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.5548 - acc_top_1: 0.9999 - val_loss: 2.7670 - val_acc_top_1: 0.5969
Epoch 00151: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 152/200
lr =  0.004000000000000001

Epoch 00152: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.5437 - acc_top_1: 1.0000 - val_loss: 2.8120 - val_acc_top_1: 0.5988
Epoch 00152: val_acc_top_1 did not improve from 0.60738

========================================================================

Epoch 153/200
lr =  0.004000000000000001

Epoch 00153: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.5331 - acc_top_1: 0.9999 - val_loss: 2.3472 - val_acc_top_1: 0.6123
Epoch 00153: val_acc_top_1 improved from 0.60738 to 0.61231, saving model to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/checkpoints/checkpoint_best_weights_153.h5

========================================================================

Epoch 154/200
lr =  0.004000000000000001

Epoch 00154: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.5228 - acc_top_1: 0.9999 - val_loss: 2.5040 - val_acc_top_1: 0.5975
Epoch 00154: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 155/200
lr =  0.004000000000000001

Epoch 00155: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 202s - loss: 0.5126 - acc_top_1: 0.9999 - val_loss: 2.7310 - val_acc_top_1: 0.5942
Epoch 00155: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 156/200
lr =  0.004000000000000001

Epoch 00156: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 202s - loss: 0.5024 - acc_top_1: 0.9998 - val_loss: 2.6728 - val_acc_top_1: 0.5883
Epoch 00156: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 157/200
lr =  0.004000000000000001

Epoch 00157: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.4927 - acc_top_1: 0.9999 - val_loss: 2.4121 - val_acc_top_1: 0.6012
Epoch 00157: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 158/200
lr =  0.004000000000000001

Epoch 00158: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.4830 - acc_top_1: 0.9999 - val_loss: 2.6727 - val_acc_top_1: 0.6037
Epoch 00158: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 159/200
lr =  0.004000000000000001

Epoch 00159: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.4737 - acc_top_1: 0.9999 - val_loss: 2.2399 - val_acc_top_1: 0.5975
Epoch 00159: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 160/200
lr =  0.004000000000000001

Epoch 00160: LearningRateScheduler setting learning rate to 0.004000000000000001.
 - 201s - loss: 0.4645 - acc_top_1: 0.9998 - val_loss: 2.3946 - val_acc_top_1: 0.5988
Epoch 00160: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 161/200
lr =  0.0008000000000000003

Epoch 00161: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4586 - acc_top_1: 0.9999 - val_loss: 1.7759 - val_acc_top_1: 0.5978
Epoch 00161: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 162/200
lr =  0.0008000000000000003

Epoch 00162: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4567 - acc_top_1: 0.9998 - val_loss: 2.0134 - val_acc_top_1: 0.6083
Epoch 00162: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 163/200
lr =  0.0008000000000000003

Epoch 00163: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4548 - acc_top_1: 0.9999 - val_loss: 2.4117 - val_acc_top_1: 0.6015
Epoch 00163: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 164/200
lr =  0.0008000000000000003

Epoch 00164: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4530 - acc_top_1: 0.9999 - val_loss: 1.7739 - val_acc_top_1: 0.6040
Epoch 00164: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 165/200
lr =  0.0008000000000000003

Epoch 00165: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4512 - acc_top_1: 1.0000 - val_loss: 2.2737 - val_acc_top_1: 0.5997
Epoch 00165: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 166/200
lr =  0.0008000000000000003

Epoch 00166: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4492 - acc_top_1: 1.0000 - val_loss: 2.7752 - val_acc_top_1: 0.5923
Epoch 00166: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 167/200
lr =  0.0008000000000000003

Epoch 00167: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4475 - acc_top_1: 1.0000 - val_loss: 2.2928 - val_acc_top_1: 0.6052
Epoch 00167: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 168/200
lr =  0.0008000000000000003

Epoch 00168: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4459 - acc_top_1: 0.9999 - val_loss: 2.3462 - val_acc_top_1: 0.5972
Epoch 00168: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 169/200
lr =  0.0008000000000000003

Epoch 00169: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4438 - acc_top_1: 0.9999 - val_loss: 2.0908 - val_acc_top_1: 0.5938
Epoch 00169: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 170/200
lr =  0.0008000000000000003

Epoch 00170: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4421 - acc_top_1: 0.9999 - val_loss: 2.3051 - val_acc_top_1: 0.5997
Epoch 00170: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 171/200
lr =  0.0008000000000000003

Epoch 00171: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4403 - acc_top_1: 0.9999 - val_loss: 2.9894 - val_acc_top_1: 0.5923
Epoch 00171: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 172/200
lr =  0.0008000000000000003

Epoch 00172: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4387 - acc_top_1: 0.9999 - val_loss: 2.6986 - val_acc_top_1: 0.6086
Epoch 00172: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 173/200
lr =  0.0008000000000000003

Epoch 00173: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4370 - acc_top_1: 1.0000 - val_loss: 2.1247 - val_acc_top_1: 0.6074
Epoch 00173: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 174/200
lr =  0.0008000000000000003

Epoch 00174: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4351 - acc_top_1: 1.0000 - val_loss: 2.0637 - val_acc_top_1: 0.6006
Epoch 00174: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 175/200
lr =  0.0008000000000000003

Epoch 00175: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4334 - acc_top_1: 0.9999 - val_loss: 1.6665 - val_acc_top_1: 0.6055
Epoch 00175: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 176/200
lr =  0.0008000000000000003

Epoch 00176: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4319 - acc_top_1: 0.9998 - val_loss: 2.7255 - val_acc_top_1: 0.6018
Epoch 00176: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 177/200
lr =  0.0008000000000000003

Epoch 00177: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4299 - acc_top_1: 0.9999 - val_loss: 2.2901 - val_acc_top_1: 0.6086
Epoch 00177: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 178/200
lr =  0.0008000000000000003

Epoch 00178: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4283 - acc_top_1: 0.9999 - val_loss: 2.0913 - val_acc_top_1: 0.6006
Epoch 00178: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 179/200
lr =  0.0008000000000000003

Epoch 00179: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4266 - acc_top_1: 1.0000 - val_loss: 2.8383 - val_acc_top_1: 0.5997
Epoch 00179: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 180/200
lr =  0.0008000000000000003

Epoch 00180: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4251 - acc_top_1: 0.9999 - val_loss: 2.9274 - val_acc_top_1: 0.5972
Epoch 00180: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 181/200
lr =  0.0008000000000000003

Epoch 00181: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4231 - acc_top_1: 1.0000 - val_loss: 2.9293 - val_acc_top_1: 0.5935
Epoch 00181: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 182/200
lr =  0.0008000000000000003

Epoch 00182: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4215 - acc_top_1: 1.0000 - val_loss: 2.1829 - val_acc_top_1: 0.5978
Epoch 00182: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 183/200
lr =  0.0008000000000000003

Epoch 00183: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4200 - acc_top_1: 1.0000 - val_loss: 2.5160 - val_acc_top_1: 0.6071
Epoch 00183: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 184/200
lr =  0.0008000000000000003

Epoch 00184: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4181 - acc_top_1: 1.0000 - val_loss: 2.3670 - val_acc_top_1: 0.6037
Epoch 00184: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 185/200
lr =  0.0008000000000000003

Epoch 00185: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4166 - acc_top_1: 0.9999 - val_loss: 2.2722 - val_acc_top_1: 0.6015
Epoch 00185: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 186/200
lr =  0.0008000000000000003

Epoch 00186: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4150 - acc_top_1: 1.0000 - val_loss: 2.5575 - val_acc_top_1: 0.6065
Epoch 00186: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 187/200
lr =  0.0008000000000000003

Epoch 00187: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4134 - acc_top_1: 0.9999 - val_loss: 1.8238 - val_acc_top_1: 0.6098
Epoch 00187: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 188/200
lr =  0.0008000000000000003

Epoch 00188: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4117 - acc_top_1: 0.9999 - val_loss: 2.4420 - val_acc_top_1: 0.6003
Epoch 00188: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 189/200
lr =  0.0008000000000000003

Epoch 00189: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 202s - loss: 0.4101 - acc_top_1: 0.9999 - val_loss: 2.2309 - val_acc_top_1: 0.6040
Epoch 00189: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 190/200
lr =  0.0008000000000000003

Epoch 00190: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 202s - loss: 0.4085 - acc_top_1: 1.0000 - val_loss: 1.6694 - val_acc_top_1: 0.6006
Epoch 00190: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 191/200
lr =  0.0008000000000000003

Epoch 00191: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4068 - acc_top_1: 1.0000 - val_loss: 2.9618 - val_acc_top_1: 0.6034
Epoch 00191: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 192/200
lr =  0.0008000000000000003

Epoch 00192: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4052 - acc_top_1: 1.0000 - val_loss: 2.0842 - val_acc_top_1: 0.6018
Epoch 00192: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 193/200
lr =  0.0008000000000000003

Epoch 00193: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4035 - acc_top_1: 1.0000 - val_loss: 1.8617 - val_acc_top_1: 0.6065
Epoch 00193: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 194/200
lr =  0.0008000000000000003

Epoch 00194: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.4020 - acc_top_1: 0.9999 - val_loss: 2.1757 - val_acc_top_1: 0.5914
Epoch 00194: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 195/200
lr =  0.0008000000000000003

Epoch 00195: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 202s - loss: 0.4005 - acc_top_1: 0.9999 - val_loss: 1.7649 - val_acc_top_1: 0.6028
Epoch 00195: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 196/200
lr =  0.0008000000000000003

Epoch 00196: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.3988 - acc_top_1: 0.9999 - val_loss: 2.4793 - val_acc_top_1: 0.6000
Epoch 00196: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 197/200
lr =  0.0008000000000000003

Epoch 00197: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.3973 - acc_top_1: 1.0000 - val_loss: 2.3901 - val_acc_top_1: 0.6009
Epoch 00197: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 198/200
lr =  0.0008000000000000003

Epoch 00198: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.3957 - acc_top_1: 1.0000 - val_loss: 2.4434 - val_acc_top_1: 0.6037
Epoch 00198: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 199/200
lr =  0.0008000000000000003

Epoch 00199: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 202s - loss: 0.3944 - acc_top_1: 0.9999 - val_loss: 2.5604 - val_acc_top_1: 0.6092
Epoch 00199: val_acc_top_1 did not improve from 0.61231

========================================================================

Epoch 200/200
lr =  0.0008000000000000003

Epoch 00200: LearningRateScheduler setting learning rate to 0.0008000000000000003.
 - 201s - loss: 0.3925 - acc_top_1: 0.9999 - val_loss: 2.3415 - val_acc_top_1: 0.6034
Epoch 00200: val_acc_top_1 did not improve from 0.61231

========================================================================

Start Time: 15:23:06 PM Friday 2020-08-28
Stop Time : 02:52:52 AM Saturday 2020-08-29
Run Time  : 11:29:46

Data Modifications:
  width_shift_range: 4.0
  height_shift_range: 4.0
  horizontal_flip: True
  featurewise_center: True
  featurewise_std_normalization: True

Notes:
  Expt Notes: Used to create src expts for tiny imagenet notliving vs living.

Peak Accuracy: 61.23% at epoch 153

Closing log results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/metadata
Saving log file to results/opt_tfer_expts/tinyimagenet_200_notliving_living_expts/wide_resnet_28_10_arch/src_nets/workshop_expts/alt08.arl.army.mil_v0/metadata/Expt_output.log
(dnn_py3) sgutstei@alt08 ~/Projects/opt-tfer> 

